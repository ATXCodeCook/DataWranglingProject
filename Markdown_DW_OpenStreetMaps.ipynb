{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.18-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python2",
   "display_name": "Python 2",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<style>\n",
    "        a { text-decoration: none;\n",
    "            font-weight: normal;\n",
    "            color: #0000aa;\n",
    "          }\n",
    "          \n",
    "      img {\n",
    "            width: 35%;\n",
    "            height: auto;\n",
    "          }\n",
    "\n",
    "     code { font-weight: 600; }\n",
    "\n",
    "     h1,h2,h3,h4,h5,h6 { font-weight: 500; }\n",
    "        \n",
    "</style>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Data Wrangling: OpenStreetMaps Data Case Study\n",
    "### Patrick Cook  03/11/2021\n",
    "\n",
    "![Round Rock City Boundary](images/round_rock_map.JPG \"Round Rock city boundary\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Map Area\n",
    "Round Rock, TX, United States\n",
    "* https://www.openstreetmap.org/relation/115318\n",
    "\n",
    "* https://overpass-api.de/api/map?bbox=-97.8181,30.4570,-97.5267,30.5957\n",
    "\n",
    "The Overpass API link was used to download the 220 MB osm file for Round Rock. The map file was placed in the [data folder](data \"Link to Folder\") and renamed to round_rock.xml.\n",
    "\n",
    "This area was chosen because I am a resident so I have some domain knowledge. I am interested in finding out how well Round Rock is represented in OpenStreetMaps and the accuracy of the data. Tourism is important to my city's revenue and OpenStreetMaps is used when creating boundaries to measure tourism visitation patterns. In addition, businesses benefit from additional free sources of information that may drive customers to them. Therefore, it is important that they are represented in the data and the information is accurate and up-to-date. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Exploration and Issues Discovered in the Round Rock Data\n",
    "**Process**: During the exploration phase, a sample of the data [round_rock_sample](data \"Link to folder\") was created using the create_sample function in the [explore_raw_data module](modules \"Link to folder\") to test functions on and better understand the structure of the data. The [`count_tags()`](modules\\explore_raw_data.py \"Link to containing .py file\") was used to get all tag types found in the complete [round_rock.xml](data \"Link to folder\") file including number of occurences.\n",
    "\n",
    "**Findings**: The original data contains 4 header tags (osm, meta, note and bounds) containing file information such as download date, time and position. Then it is followed by node, way and relation parent elements. The parent elements contain data entry and location information as attributes. The parents have child elements member, nd and tag. The majority of the descriptive information is coded in the 'tag' child element attributes. The elements found and the count of each is given below. The majority of tags are node and nd containing position and data entry information. The tag elements contain the descriptive information that will be used in the analysis. Therefore, the focus will be on extracting and structuring this data.\n",
    "\n",
    "### Element Tag Types\n",
    "    \n",
    "    {\n",
    "      'bounds': 1,\n",
    "      'member': 19047,\n",
    "      'meta': 1,\n",
    "      'nd': 1152020,\n",
    "      'node': 1027359,\n",
    "      'note': 1,\n",
    "      'osm': 1,\n",
    "      'relation': 357,\n",
    "      'tag': 328734,\n",
    "      'way': 112084\n",
    "    }\n",
    "\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Exploration of Attributes\n",
    "To get an idea of the type, number and structure of tag's key attributes, [`categorize_tag_key_characters()`](modules\\explore_raw_data.py \"Link to containing .py file\") was run. It was found that there are 248 unique tag keys that are all lower case, 189 tag keys that have a colon and are lower and 69 unique tag keys that contain capital letters or contain multiple colons. There are no tag keys with special problematic characters. The summary results are below.\n",
    "\n",
    "``` python\n",
    "{'problemchars': 0, 'lower': 226175, 'other': 1295, 'lower_colon': 101264}\n",
    "```\n",
    "\n",
    "```\n",
    "There are:\n",
    "          248 unique keys in lower,\n",
    "          189 unique keys in lower_colon,\n",
    "          0 unique keys in problemchars and\n",
    "          69 unique keys in other.\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Auditing the Data\n",
    "Reviewing the sample file data,  errors and non-standard data were found in many of the attribute key categories. I decided to focus on addr:street (street names), addr:postcode (postal codes) and phone (phone numbers) for this project. The street name and phone numbers are a focus as businesses need this information to be accurate for customers to contact them. The postal code was picked due to some outliers being found that are not postcodes of the city area."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Auditing Adress Street Names  \n",
    "**Process**: The audit street name function from the Udacity Data Wrangling course was modified to catch street names with special characters. The returned dictionary information was reviewed and used to modify the mapping dictionary and to update the expected_list of street type endings. The [`audit_street_validate_corrections()`](modules\\audit_streets.py \"Link to containing .py file\") was modified to use the mapping dictionary to test corrections to the street names. \n",
    "\n",
    "**Findings**: The errors found during the street auditing process were:\n",
    "* Streets with missing or non-standard street type endings.\n",
    "    * Street names ending with Cv, Cv. and Cove\n",
    "    * Street names ending with \"Suite 301\" or \"United States\"\n",
    "* Non-standard formating for highways and directions.\n",
    "    * IH-35, IH35, Interstate 35, I-35, Highway 35\n",
    "    * North, N, N. and directions appearing at end of street name after street type \n",
    "* Steet names containing partial or full addresses such as\n",
    "    * house number or apartment number\n",
    "    * complete address written out with postal code, state and country\n",
    "\n",
    "<br>\n",
    "The following code are examples of street names found during auditing.\n",
    "\n",
    "\n",
    "```   {\n",
    "       ',': set(['11066 Pecan Park St  300, Cedar Park, TX',\n",
    "                 '1335 E Whitestone Blvd T100, Cedar Park, TX 78613, United States',\n",
    "                 'Louis Henna Blvd, TX 45 Frontage Road',\n",
    "                 'N Interstate Hwy 35, Round Rock, TX 78681',\n",
    "                 'S Interstate 35, #260']),\n",
    "       '35': set(['Highway Interstate 35',\n",
    "                  'N Interstate Hwy 35',\n",
    "                  'North IH 35',\n",
    "                  'North Interstate 35',\n",
    "                  'S Interstate 35',\n",
    "                  'S Interstate Highway 35',\n",
    "                  'South Interstate 35']),\n",
    "       685': set(['FM 685', 'Farm to Market 685']),\n",
    " 'Barrhead': set(['Barrhead']),\n",
    "       'Cv': set(['Copper Point Cv',\n",
    "                  'Quiet Meadows Cv',\n",
    "                  'Ripley Castle Cv',\n",
    "                  'Secluded Willow Cv']),\n",
    "      }            \n",
    "```\n",
    "To clean the street names, all corrections were added to one mapping dictionary so that specific, single instances needing cleaning would occur in the same step as the general programmatic cleaning of the street endings. This **caused a problem with partial cleanning of the street name** instead of the whole street name being cleaned. As an example, the street names \"N. IH35\" and \"IH35\" are both street names found in the data. The mapping contains these \"N. IH35\" : \"North I-35\" and also \"IH35\" : \"I-35\" to catch these instances. Since dictionaries are not ordered in Python 2.7, the N. IH35 was being caught by the \"IH35\" mapping key and converted to N. I-35 (partially cleaned) instead of \"North I-35\".\n",
    "\n",
    "To fix the issue, a separate function [`dictionary_key_length_ordered_descending()`](modules\\helper_functions.py \"Link to containing .py file\") was used to create an OrderedDict by key value length. This ensures the longer, more specific keys are found before the less specific keys. Separating out the mapping dictionaries and sending the data to both dictionaries would be another option that might reduce time and spacial complexity but would require further testing. Testing the function gave the expected results but did highlight issues with street names with valid endings but beginning with abreviation letters for directions (N, S, E, W). The cleaning function added a directions_mapping to check for this case. Examples of the cleaning function results during auditing are given below. \n",
    "\n",
    "### Sample Street Name Cleaning Results:\n",
    "```\n",
    "11066 Pecan Park Ste 300, Cedar Park, TX => Pecan Park Boulevard\n",
    "S Bell Blvd., Suite 301 => South Bell Boulevard\n",
    "U.S. 183 => US 183\n",
    "1500 S. IH35 => South I-35\n",
    "N. IH35, => North I-35\n",
    "W. Parmer Lane => West Parmer Lane\n",
    "N. IH 35 Pflugerville => North I-35\n",
    "MCNEIL RD => McNeil Road\n",
    "University Blvd => University Boulevard\n",
    "Exchange Blvd => Exchange Boulevard\n",
    "N Heatherwilde Blvd => N Heatherwilde Boulevard\n",
    "E Palm Valley Blvd => E Palm Valley Boulevard\n",
    "South Bell Blvd => South Bell Boulevard\n",
    "200 University Blvd => University Boulevard\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Auditing Postal Codes\n",
    "The postal codes were audited next using the [`audit_postacode()`](modules\\audit_postcodes.py \"Link to containing .py file\") function. Post codes in the United States are comprised of 5 digits with the first digit giving the National Area, the next two digits giving a Sectional (Regional) Area and the last two digits giving the Delivery Area. Therefore all postcodes should be 5 digits. To validate the data, postcodes for the City of Round Rock were placed in a list to compare postcode values to. The results of the audit showed that all postal codes for the city were found in the data file. There were also 10 postal codes that were not listed as belonging to Round Rock. The results of the audit is shown below.\n",
    "\n",
    "```\n",
    "There was at least one instance of each Round Rock postcode found in the data.\n",
    "The following postcodes are not Round Rock postcodes and need review.\n",
    "    set(['78613', '78758', '78680', '78682', '78621', '78729', '787664', '78641', '78728-1275', '78750'])\n",
    "Completed\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Validating the postcode data\n",
    "Using [Round Rock's City GIS software](https://maps.roundrocktexas.gov/cityview/),  researching and exploring specific elements, the following information was discovered.\n",
    "* The postcodes 78613, 78758, 78729, 78641 and 78750 are neighborhoods at the border of the city\n",
    "* The postcode 78621 does not seem to be touching the border of Round Rock and will require further investigation.\n",
    "* The postcodes 78680 and 78682 are listed as specially designated zoning regions used for PO Box, high volume or historical significance postcodes.\n",
    "* The postcode 787664 is a typo and should be 78664. This will be verified using the elements street, longitude and latitude data.\n",
    "* The postcode 78728-1275 has the expanded code added (ZIP+4). The expanded code will be removed during cleaning."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Running the [`explore_postcode_details()`](modules\\audit_postcodes.py \"Links to containing .py file\") function on the two zip codes showed both street addresses are in Round Rock and will be corrected using a mapping dictionary during cleaning.\n",
    "* The 78621 postcode (Hoody's) should be 78681 and is likely a typo (2 instead of 8 on keypad).\n",
    "* The 787664 postcode (Self Storage Facility) is a typo and should be 78664."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Audit Phone Numbers\n",
    "Auditing the phone numbers using the [`audit_phones()`](modules\\audit_phones.py \"Link to containing .py file\") function shows mostly conisitent data with variations between using dashes, spaces and parenthesis to delimenate the numbers. Other issues that were discovered are:\n",
    "* Numbers using more that one deliminator such as spaces and parenthesis\n",
    "* Numbers using no deliminators\n",
    "* Unicode character code /2100 found intead of '-'\n",
    "* Four numbers where found more than once in the data\n",
    "\n",
    "The results of the phone audit are below.\n",
    "\n",
    "```\n",
    "All phone numbers were checked.\n",
    "\n",
    "There are 45 malformed numbers, 4 duplicate numbers and 248 wellformed numbers.\n",
    "\n",
    "The malformed numbers found are: \n",
    "```\n",
    "\n",
    "```python\n",
    "{'(512) 246-7941'       : 'fix_number',\n",
    " '+1 (512) 469-7000'    : 'fix_number',\n",
    " '+1 (512) 759-5900'    : 'fix_number',\n",
    " '+1 512 218 5062'      : 'fix_number',\n",
    " '+1 512 218 9888'      : 'fix_number',\n",
    " '+1 512 238 0820'      : 'fix_number',\n",
    " '+1 512 244 3737'      : 'fix_number',\n",
    " '+1 512 248 7000'      : 'fix_number',\n",
    " '+1 512 252 1133'      : 'fix_number',\n",
    " '+1 512 255 7000'      : 'fix_number',\n",
    " '+1 512 255 7530'      : 'fix_number',\n",
    " '+1 512 258 8114'      : 'fix_number',\n",
    " '+1 512 277 6959'      : 'fix_number',\n",
    " '+1 512 310 7600'      : 'fix_number',\n",
    " '+1 512 310 7678'      : 'fix_number',\n",
    " '+1 512 324 4000'      : 'fix_number',\n",
    " '+1 512 341 1000'      : 'fix_number',\n",
    " '+1 512 362 9525'      : 'fix_number',\n",
    " '+1 512 402 7811'      : 'fix_number',\n",
    " '+1 512 528 7000'      : 'fix_number',\n",
    " '+1 512 532 2200'      : 'fix_number',\n",
    " '+1 512 600 0145'      : 'fix_number',\n",
    " '+1 512 637 6890'      : 'fix_number',\n",
    " '+1 512 733 9660'      : 'fix_number',\n",
    " '+1 512 990 5413'      : 'fix_number',\n",
    " '+1 512)351 3179'      : 'fix_number',\n",
    " '+1 512-244-8500'      : 'fix_number',\n",
    " '+1 512-260-5443'      : 'fix_number',\n",
    " '+1 512-260-6363'      : 'fix_number',\n",
    " '+1 512-310-8952'      : 'fix_number',\n",
    " '+1 512-338-8805'      : 'fix_number',\n",
    " '+1 512-341-7387'      : 'fix_number',\n",
    " '+1 512-421-5911'      : 'fix_number',\n",
    " '+1 512-535-5160'      : 'fix_number',\n",
    " '+1 512-535-6317'      : 'fix_number',\n",
    " '+1 512-733-6767'      : 'fix_number',\n",
    " '+1 512-851-8777'      : 'fix_number',\n",
    " '+1 737 757 3100'      : 'fix_number',\n",
    " '+1512-413-9671'       : 'fix_number',\n",
    " '+1512-909-2528'       : 'fix_number',\n",
    " '+15123885728'         : 'fix_number',\n",
    " '+15124282300'         : 'fix_number',\n",
    " '+15124648382'         : 'fix_number',\n",
    " '1+512-696-5209'       : 'fix_number'}\n",
    "\n",
    "There were 4 duplicate items found: \n",
    "\n",
    "set(['+1-512-428-2500', '+1-512-336-1328', '+1-512-310-8791', '+1-512-238-0475'])\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Cleaning the Phone Numbers\n",
    "To make the data more consistent, it was decided to follow the US phone pattern with leading country code separated by dashes ( +1-###-###-#### ). The numbers that did not follow the US phone pattern were added as a key to the malformed dictionary. A partial clean was first performed using the [`phone_partial_clean()`](modules\\helper_functions.py \"Link to conataining .py file\") function. This function programatically cleans the data by using the `replace()` string method calls to replace:\n",
    "\n",
    "* All blanks with hyphens,\n",
    "* Removes all parenthesis\n",
    "\n",
    "Of the 45 original phone number corrections, the new dictionary still contained 8 phone numbers with errors (see below). These could be programatically cleaned using specific algorithms for each. The time complexity would increase significantly for so few corrections needed. Therefore, it was decided to manually edit these numbers in the mapping dictionary.\n",
    "\n",
    "Numbers not fixed by partial clean (manually fixed):\n",
    "\n",
    "```\n",
    " '(512) 246-7941'       : '512-246-7941',\n",
    " '+1 512)351 3179'      : '+1-512351-3179',\n",
    " u'+1-737-484\\u20110700': u'+1-737-484\\u20110700',\n",
    " '+1512-413-9671'       : '+1512-413-9671',\n",
    " '+1512-909-2528'       : '+1512-909-2528',\n",
    " '+15123885728'         : '+15123885728',\n",
    " '+15124282300'         : '+15124282300',\n",
    " '+15124648382'         : '+15124648382',\n",
    " \n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Creating CSV Files and Database Import\n",
    "With all the planned cleaning verified, the data was converted to csv files using the [`xml_to_csv()`](data.py \"Link to containing .py file.\") function in the data.py file. To identify attributes to be cleaned, the Tag element attributes are checked to see if they are in the update_list. If the attribute key is one of the attributes to be cleaned, they are sent to the [`update_values()`](modules\\update_values.py \"Link to containing .py file.\") function which acts as flow control and sends the value to the appropriate cleaning function returning the cleaned value.\n",
    "\n",
    "``` python\n",
    "...\n",
    "UPDATE_LIST = ['addr:street', 'addr:postcode', 'phone']\n",
    "...\n",
    "    for child in element.iter('tag'):\n",
    "        tag_dict = {}\n",
    "\n",
    "        if child.attrib['k'] in UPDATE_LIST:                                        \n",
    "            child.attrib['v'] = update_value(child.attrib['k'], child.attrib['v'])\n",
    "...\n",
    "```\n",
    "\n",
    "During the conversion to csv, the **entire dataset was validated using the cerberus package** and the [schema.py](schema.py \"Link to .py file\") file. Additionally, after the conversion to csv, the csv files were descriminately audited using filters to confirm corrections.\n",
    "\n",
    "**Note**: Due to the time complexity of running the cerberus validation, smaller test.xml and round_rock_sample.xml files were run prior to validating the entire data set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## SQL Import\n",
    "After converting to csv files, the data was then loaded into an Sqlite3 database, [Round_RockDb.db](sql \"Link opens containing folder\"), and the function [`process_sql()`](modules\\process_sql.py \"Link Opens containing .py file\") was used to create the tables for the database. Included in the [`process_sql()`](modules\\process_sql.py \"Link Opens containing .py file\") function, is the table schema used to create the tables. The table schema is also available in the sql folder in the [create_table_schema](sql\\create_table_schema.py \"Link opens containing file\") file for convenience.  \n",
    "\n",
    "Finally, the csv files were loaded into the tables using the following code and the [`csv_to_sql()`](modules\\process_sql.py \"Link opens containing .py file.\") function.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "``` python\n",
    "    # import csv to sqlite db tables: nodes_tags, nodes, ways_nodes, ways_tags, ways\n",
    "\n",
    "    from modules.process_sql import csv_to_sql\n",
    "\n",
    "    # Dictionary of file paths and table names\n",
    "    csv_file_table ={'sql\\\\csv\\\\nodes_tags.csv' : 'nodes_tags', \n",
    "                     'sql\\\\csv\\\\nodes.csv' : 'nodes', \n",
    "                     'sql\\\\csv\\\\ways_nodes.csv' : 'ways_nodes', \n",
    "                     'sql\\\\csv\\\\ways_tags.csv' : 'ways_tags', \n",
    "                     'sql\\\\csv\\\\ways.csv' : 'ways'\n",
    "                    }\n",
    "\n",
    "    db_file = 'sql\\\\Round_RockDb.db'\n",
    "\n",
    "    for csv_file, db_table in csv_file_table.items():\n",
    "        csv_to_sql(csv_file, db_file, db_table)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Database Queries: Statistical Overview of Dataset\n",
    "The following sections contain information about the Size of the files compared to the database, number of unique users, number of nodes and ways, and number of:\n",
    "* unique street names\n",
    "* unique postcodes and \n",
    "* unique phone numbers\n",
    "\n",
    "In addition, I will look at the occurrence counts in the data and the top 10 in the street names and postcode groups to find which are most represented in the data. I will also examine the phone numbers to determine which numbers appeared more than once in the data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Size of Files\n",
    "The [`get_file_info()`](modules\\helper_functions.py \"Link to containing .py file\") helper function was used to gather information about specific files used. The original data was 220 MB in size and the database has reduces this size by 101 MB to 119 MB. The total of all the csv files are 135 MB.\n",
    "\n",
    "\n",
    "```\n",
    "    round_rock.xml...............220 MB\n",
    "    round_rockdb.db..............119 MB\n",
    "    nodes_tags.csv.................1 MB\n",
    "    nodes.csv.....................92 MB\n",
    "    ways_nodes.csv................26 MB\n",
    "    ways_tags.csv..................9 MB\n",
    "    ways.csv.......................7 MB\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Number of Unique Users\n",
    "\n",
    "``` sql\n",
    "    SELECT COUNT(DISTINCT (wnUnion.uid)) as \"Total Unique Users\" \n",
    "    FROM (SELECT uid FROM ways UNION ALL SELECT uid FROM nodes) as wnUnion;\n",
    "```\n",
    "\n",
    "Total Unique Users: **1054**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Top 10 Users Represented in Data\n",
    "The top 10 users are listed below. The data shows at least one user likely using two user ids and accounting for the top 2 appearances. The suffix \"*_atxbuildings*\" was found to be from an [openstreetmap guide](https://wiki.openstreetmap.org/wiki/Austin,_TX/Buildings_Import/Software_Setup) on an Austin, TX wiki page.\n",
    "<br>\n",
    "\n",
    "``` sql\n",
    "    SELECT user, uid, COUNT(wnUnion.uid) as \"Appearance Count\" \n",
    "                   FROM (SELECT user, uid FROM ways \n",
    "                         UNION ALL \n",
    "                         SELECT user, uid FROM nodes) as wnUnion\n",
    "                   GROUP BY user, uid\n",
    "                   ORDER BY \"Appearance Count\" DESC\n",
    "                   LIMIT 10;\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "|           user             |   |     uid     |   |  Appearance Count  |\n",
    "|        ---------:          | - |  ---------: | - | -----------------: |\n",
    "|  ccjjmartin__atxbuildings  |   |  3405475    |   |      312213        |\n",
    "|   ccjjmartin_atxbuildings  |   |  3370181    |   |      309281        |\n",
    "|    patisilva_atxbuildings  |   |  3369502    |   |      122028        |\n",
    "|            SathyaPendyala  |   |  3618405    |   |       30931        |\n",
    "|                     s0707  |   | 11358207    |   |       25687        |\n",
    "|           woodpeck_fixbot  |   |   147510    |   |       24508        |\n",
    "|       wilsaj_atxbuildings  |   |  3341346    |   |       24321        |\n",
    "|                technogeek  |   |    98830    |   |       18930        |\n",
    "|                    torapa  |   |  5446055    |   |       11552        |\n",
    "|                   JanineG  |   | 12179240    |   |       11016        |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "The \"**_atxbuildings** reference was found in 5 users including the duplicated user cjmartin. Also, checking for other user names with \"cjmartin\" showed no other users with that name in their user name.  \n",
    "<br>\n",
    "\n",
    "\n",
    "``` sql\n",
    "    SELECT user, uid, COUNT(wnUnion.uid) as \"Appearance Count\" \n",
    "                   FROM (SELECT user, uid FROM ways \n",
    "                         UNION ALL \n",
    "                         SELECT user, uid FROM nodes) as wnUnion\n",
    "                   WHERE user LIKE '%atxbuildings%'\n",
    "                   GROUP BY user, uid\n",
    "                   ORDER BY \"Appearance Count\" DESC;\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "|           user             |   |     uid     |   |  Appearance Count  |\n",
    "|        ---------:          | - |  ---------: | - | -----------------: |\n",
    "|  ccjjmartin__atxbuildings  |   |  3405475    |   |      312213        |\n",
    "|   ccjjmartin_atxbuildings  |   |  3370181    |   |      309281        |\n",
    "|    patisilva_atxbuildings  |   |  3369502    |   |      122028        |\n",
    "|       wilsaj_atxbuildings  |   |  3341346    |   |       24321        |\n",
    "|  lyzidiamond_atxbuildings  |   |  3409435    |   |        2292        |\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Number of Nodes and Ways\n",
    "\n",
    "The **nodes** table has **1027359 rows** and the **ways** table has **112084 rows**. \n",
    "\n",
    "```\n",
    "    SELECT count(*) as \"Nodes Count\" FROM nodes;\n",
    "    SELECT count(*) as \"Ways Count\" FROM ways;\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Number of Streets, Postcodes and Phone Numbers  \n",
    "The results of streets, postcodes and phones in the data are:\n",
    "\n",
    "|     keys  |      |   Count   |\n",
    "|    ----:  | ---  |  -------: |\n",
    "|   street  |      |   28533   |\n",
    "| postcode  |      |    7110   |\n",
    "|    phone  |      |     308   |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "``` sql\n",
    "SELECT key, COUNT(wnUnion.value) as \"Appearance Count\" \n",
    "                    FROM (SELECT key, value, type FROM ways_tags \n",
    "                          UNION ALL \n",
    "                          SELECT key, value, type FROM nodes_tags) as wnUnion\n",
    "                    WHERE (type = 'addr' AND key IN ('street', 'postcode')) OR\n",
    "                                 (type = 'regular' AND key = 'phone')\n",
    "                    GROUP BY key\n",
    "                    ORDER BY \"Appearance Count\" DESC;\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Top 10 Streets by Occurence in the Data\n",
    "The top 10 streets by occurence are:  \n",
    "\n",
    "**value**|    |**Unique Street Count**\n",
    ":----- | --- |:-----:\n",
    "Winding Shore Lane|   |142\n",
    "East Whitestone Boulevard|   |135\n",
    "Pencil Cactus Drive|   |131\n",
    "Dashwood Creek Drive|   |127\n",
    "Brushy Creek Road|   |117\n",
    "Derby Day Avenue|   |116\n",
    "Dorman Drive|   |110\n",
    "Loch Linnhe Loop|   |109\n",
    "Tortoise Street|   |106\n",
    "Farm Pond Lane|   |104\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "```  sql\n",
    "    SELECT value, COUNT(wnUnion.value) as \"Unique Street Count\" \n",
    "                        FROM (SELECT key, value, type FROM ways_tags \n",
    "                            UNION ALL \n",
    "                            SELECT key, value, type FROM nodes_tags) as wnUnion\n",
    "                        WHERE key = 'street' AND type = 'addr'\n",
    "                        GROUP BY value\n",
    "                        ORDER BY \"Unique Street Count\" DESC\n",
    "                        Limit 10;\n",
    "\n",
    "```\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Of the top 10 streets only **East Whitestone Blvd**, **Brushy Creek Road**, **Dorman Drive** and **Lock Linnhe Loop** are considered to be in the actual Round Rock city limits. The remaining 7 are in the map selection bounding box but are not officially in Round Rock's city limits. All streets except East Whitestone Blvd and Brushy Creek Road are residential roads. Based on this preliminary data, Round Rock businesses do not seem to be well represented in the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Top 10 Postcodes Represented in Data\n",
    "\n",
    "The following are the results of the top 10 postcodes by occurence. The bolded are either on the border or have a small area in the Round Rock city limits. The results show that a majority of the postcode data represents the Round Rock area. Further future study will be needed to find out if they are businesses or residential.\n",
    "\n",
    "**Postcode**|   |**Count**\n",
    ":-----:| --- |:-----:\n",
    "78660|  |  1839\n",
    "**78613**|  |  **1744**\n",
    "78717|  |  755\n",
    "78664|  |  558\n",
    "78681|  |  460\n",
    "78728|  |  453\n",
    "**78729**|  |  **367**\n",
    "78634|  |  293\n",
    "**78641**|  |  **221**\n",
    "78665|  |  201\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "```\n",
    "    SELECT value, COUNT(wnUnion.value) as \"Unique Postcode Count\" \n",
    "                        FROM (SELECT key, value, type FROM ways_tags \n",
    "                            UNION ALL \n",
    "                            SELECT key, value, type FROM nodes_tags) as wnUnion\n",
    "                        WHERE key = 'postcode' AND type = 'addr'\n",
    "                        GROUP BY value\n",
    "                        ORDER BY \"Unique Postcode Count\" DESC\n",
    "                        Limit 10;\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Duplicate Phone Numbers\n",
    "There were only 4 duplicated phone numbers in the data. The results are below. One of the duplicated numbers is for a middle school, one for a business wholesaler and the remaining two are for two CVS Pharmacy stores and two for Walgreens Pharmacy stores. This further leads to the conclusion that businesses may not be well represented in the data. Next I will check how many addresses contain I-35, SH 45, Louis Henna Boulevard and 620, which are all streets with primarily businesses.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Phone Number** |     |**Count**\n",
    ":-----           | --- |:-----:\n",
    "+1-512-428-2500  |     |  2\n",
    "+1-512-336-1328  |     |  2\n",
    "+1-512-310-8791  |     |  2\n",
    "+1-512-238-0475  |     |  2\n",
    "+1-956-648-8580  |     |  1\n",
    "+1-866-874-2389  |     |  1\n",
    "+1-866-583-7952  |     |  1\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "``` sql\n",
    "    SELECT value, COUNT(wnUnion.value) as \"Unique Phone Count\" \n",
    "                        FROM (SELECT key, value, type FROM ways_tags \n",
    "                            UNION ALL \n",
    "                            SELECT key, value, type FROM nodes_tags) as wnUnion\n",
    "                        WHERE key = 'phone' AND type = 'regular'\n",
    "                        GROUP BY value\n",
    "                        ORDER BY \"Unique Phone Count\" DESC\n",
    "                        Limit 10;\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Addresses Containing I-35, SH 45, Louis Henna or 620\n",
    "The occurrences of street names with mostly business on them only show 248 records. Even more noticable from the query results below are that SH 45 shows only two business. This road is lined with businesses and should be much higher. The totals for I-35 and Ranch Road/Farm to Market 620 look more promising. There are likely several hundreds of businesses on each of these two roads but the totals for these two roads are better than SH 45. Louis Henna Boulevard's businesses are under represented. Knowing this area, there are likely over 100 businesses on this road.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Street** |       |**Occurances**\n",
    " -----:     | :-----:| -----:\n",
    "Ranch Road 620|  |82\n",
    "North I-35 |  |62\n",
    "South I-35 |  |29\n",
    "I-35 |  |22\n",
    "Louis Henna Boulevard|   |21\n",
    "West Louis Henna Boulevard|  |17\n",
    "North Ranch Road 620|  |9\n",
    "North Farm to Market 620| |4\n",
    "SH 45| |2  \n",
    "**Total**| |**248**\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "``` sql\n",
    "    SELECT value, COUNT(wnUnion.value) as \"Occurance Count\"\n",
    "                        FROM (SELECT key, value, type FROM ways_tags \n",
    "                            UNION ALL \n",
    "                            SELECT key, value, type FROM nodes_tags) as wnUnion\n",
    "                        WHERE (key = 'street' AND type = 'addr') AND\n",
    "                            (value LIKE '%I-35%' OR value LIKE '%SH 45%' OR value LIKE '%620%' or value LIKE '%Louis Henna%')\n",
    "                        GROUP BY value\n",
    "                        ORDER BY \"Occurance Count\" DESC\n",
    "                        Limit 20;\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Conclusion\n",
    "The purpose of this project was to begin exploring, auditing and cleaning the OpenStreetMaps data for Round Rock, Texas. The node and way tag attributes for streets, postcodes and phone numbers were chosen to focus on how well businesses are represented in the data. It was found that the data set does contain valid data for many businesses but is not close to a complete representation of the total businesses in the city of Round Rock. The data was found to have a high level of consistency and uniformity with the postcodes, phone numbers and street names. Though, there were instances of inconsistency, with all three attribute values they did not represent a majority of the values. There were 28,533 street entries, 7,110 postcode entries and 308 phone entries. The streets had a little over 100 corrections, the postcodes had less than 10, and the phone number corrections were less than 30. This supports that most of the data is consistent and uniform. Most important to me is the data allowed me to deepen my knowledge and skills at data wrangling by working with real world data and solving real world problems. As always, data sets can always be improved. Some ideas of ways to improve the data are given in the next section."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Additional Ideas for Improving the Data Including Benefits and Anticipated Problems\n",
    "Though most of the data is consistent and uniform, data values that should follow a strict pattern are not universally uniform. As an example, in the data there were postcodes for the US that were 6 digits or had the zip+4 format. These could be easily checked using the regex pattern `^\\d{5}$` before allowing them to be placed in the database. A similar pattern such as, `^\\+1-[2-9]\\d{2}-\\d{3}-\\d{4}$`, could be used to ensure consistency with phone numbers. If a pattern doesn't match, a user could be notified immediately and given information on formatting. Some anticipated problems with this improvement would be each country would need to have their own format pattern to check for and some mechanism would need to be in place to deal with any outliers that occur. An additional problem would be that being more strict tends to make some contributors stop contributing. Hopefully, the benefit of more consistent data would cause more businesses and developers of applications to those businesses to use the dataset. Therefore, canceling out the effect of the lost contributors. \n",
    "\n",
    "An additional thought is to make detailed standards for the naming conventions of common data values which can be programmatically checked similar to the audits done in this project. Place the standards prominently in an easily found place like the front-page menu bar. As an example of the challenge of finding a standard, while conducting a search for naming conventions I came across some OpenStreetMaps [Editing Standards and Conventions](https://wiki.openstreetmap.org/wiki/Editing_Standards_and_Conventions) through an external site but was unable to find other information on conventions used for phone numbers. When I finally found the conventions used for phone numbers, I found **two different methods** recommended as a standard. It is better to be consistent and not offer multiple ways of representing the same data. Only offer multiple choices when the data values are distinctly different such as with cuisines. Even then a menu of choices is better for the consistency and uniformity of the data. The query below gave 53 distinct cuisines found in the Round Rock data set. Some of the different representations, from the query, for the same items are:\n",
    "\n",
    "* italian_pizza, pizza\n",
    "* steak, steak_house\n",
    "* wings, Wings\n",
    "* american, American, local, regional\n",
    "* ...\n",
    "\n",
    "By offering a menu of items or at least a standard for naming items, these variations could be reduced making the data of a higher quality.\n",
    "\n",
    "```sql\n",
    "SELECT DISTINCT value\n",
    "        FROM (SELECT key, value, type FROM ways_tags \n",
    "                UNION ALL \n",
    "                SELECT key, value, type FROM nodes_tags) as wnUnion\n",
    "        WHERE (key = 'cuisine')\n",
    "        ORDER BY value;\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### References\n",
    "https://mungingdata.com/sqlite/create-database-load-csv-python/\n",
    "\n",
    "https://www.sqlitetutorial.net/sqlite-python/creating-database/\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html\n",
    "\n",
    "https://stackoverflow.com/questions/51463449/replace-csv-header-without-deleting-the-other-rows/51463964\n",
    "\n",
    "https://stackoverflow.com/questions/35486721/how-to-prevent-use-of-the-first-row-pandas-dataframe-as-column-names-when-using\n",
    "\n",
    "https://stackoverflow.com/questions/32213066/sqlite3-you-must-not-use-8-bit-bytestrings-unless-you-use-a-text-factory\n",
    "\n",
    "https://stackoverflow.com/questions/12817151/how-to-get-column-names-with-query-data-in-sqlite3\n",
    "\n",
    "https://maps.roundrocktexas.gov/cityview/\n",
    "\n",
    "https://marketbusinessnews.com/financial-glossary/zip-code/\n",
    "\n",
    "https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "\n",
    "https://thispointer.com/python-get-file-size-in-kb-mb-or-gb-human-readable-format/\n",
    "\n",
    "https://wiki.openstreetmap.org/wiki/Austin,_TX/Buildings_Import/Software_Setup\n",
    "\n",
    "https://wiki.openstreetmap.org/wiki/Editing_Standards_and_Conventions\n",
    "\n",
    "https://wiki.openstreetmap.org/wiki/Key:phone\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
