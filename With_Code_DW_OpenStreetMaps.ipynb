{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<style>\n",
    "        a { text-decoration: none;\n",
    "            font-weight: normal;\n",
    "            color: #0000aa;\n",
    "          }\n",
    "          \n",
    "      img {\n",
    "            width: 35%;\n",
    "            height: auto;\n",
    "          }\n",
    "\n",
    "     code { font-weight: 600; }\n",
    "\n",
    "     h1,h2,h3,h4,h5,h6 { font-weight: 500; }\n",
    "        \n",
    "</style>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Data Wrangling: OpenStreetMaps Data Case Study\n",
    "### Patrick Cook  03/11/2021\n",
    "\n",
    "![Round Rock City Boundary](images/round_rock_map.JPG \"Round Rock city boundary\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Map Area\n",
    "Round Rock, TX, United States\n",
    "* https://www.openstreetmap.org/relation/115318\n",
    "\n",
    "* https://overpass-api.de/api/map?bbox=-97.8181,30.4570,-97.5267,30.5957\n",
    "\n",
    "The Overpass API link was used to download the 220 MB osm file for Round Rock. The map file was placed in the [data folder](data \"Link to Folder\") and renamed to round_rock.xml.\n",
    "\n",
    "This area was chosen because I am a resident so I have some domain knowledge. I am interested in finding out how well Round Rock is represented in OpenStreetMaps and the accuracy of the data. Tourism is important to my city's revenue and OpenStreetMaps is used when creating boundaries to measure tourism visitation patterns. In addition, businesses benefit from additional free sources of information that may drive customers to them. Therefore, it is important that they are represented in the data and the information is accurate and up-to-date. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Exploration and Issues Discovered in the Round Rock Data\n",
    "**Process**: During the exploration phase, a sample of the data [round_rock_sample](data \"Link to folder\") was created using the create_sample function in the [explore_raw_data module](modules \"Link to folder\") to test functions on and better understand the structure of the data. The [`count_tags()`](modules\\explore_raw_data.py \"Link to containing .py file\") was used to get all tag types found in the complete [round_rock.xml](data \"Link to folder\") file including number of occurences.\n",
    "\n",
    "**Findings**: The original data contains 4 header tags (osm, meta, note and bounds) containing file information such as download date, time and position. Then it is followed by node, way and relation parent elements. The parent elements contain data entry and location information as attributes. The parents have child elements member, nd and tag. The majority of the descriptive information is coded in the 'tag' child element attributes. The elements found and the count of each is given below. The majority of tags are node and nd containing position and data entry information. The tag elements contain the descriptive information that will be used in the analysis. Therefore, the focus will be on extracting and structuring this data.\n",
    "\n",
    "### Element Tag Types\n",
    "    \n",
    "    {\n",
    "      'bounds': 1,\n",
    "      'member': 19047,\n",
    "      'meta': 1,\n",
    "      'nd': 1152020,\n",
    "      'node': 1027359,\n",
    "      'note': 1,\n",
    "      'osm': 1,\n",
    "      'relation': 357,\n",
    "      'tag': 328734,\n",
    "      'way': 112084\n",
    "    }\n",
    "\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'note': 1,\n",
       " 'meta': 1,\n",
       " 'bounds': 1,\n",
       " 'node': 1027359,\n",
       " 'tag': 328734,\n",
       " 'nd': 1152020,\n",
       " 'way': 112084,\n",
       " 'member': 19047,\n",
       " 'relation': 357,\n",
       " 'osm': 1}"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from modules.explore_raw_data import count_tags\n",
    "\n",
    "OSM_FILE = \"data\\\\round_rock.xml\"\n",
    "\n",
    "count_tags(OSM_FILE)"
   ]
  },
  {
   "source": [
    "## Exploration of Attributes\n",
    "To get an idea of the type, number and structure of tag's key attributes, [`categorize_tag_key_characters()`](modules\\explore_raw_data.py \"Link to containing .py file\") was run. It was found that there are 248 unique tag keys that are all lower case, 189 tag keys that have a colon and are lower and 69 unique tag keys that contain capital letters or contain multiple colons. There are no tag keys with special problematic characters. The summary results are below.\n",
    "\n",
    "``` python\n",
    "{'problemchars': 0, 'lower': 226175, 'other': 1295, 'lower_colon': 101264}\n",
    "```\n",
    "\n",
    "```\n",
    "There are:\n",
    "          248 unique keys in lower,\n",
    "          189 unique keys in lower_colon,\n",
    "          0 unique keys in problemchars and\n",
    "          69 unique keys in other.\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'lower': 226175, 'lower_colon': 101264, 'problemchars': 0, 'other': 1295}\n\nThere are:\n          248 unique keys in lower,\n          189 unique keys in lower_colon,\n          0 unique keys in problemchars and\n          69 unique keys in other.\n\n"
     ]
    }
   ],
   "source": [
    "from modules.explore_raw_data import categorize_tag_key_characters\n",
    "\n",
    "category_counts = categorize_tag_key_characters()"
   ]
  },
  {
   "source": [
    "## Auditing the Data\n",
    "Reviewing the sample file data,  errors and non-standard data were found in many of the attribute key categories. I decided to focus on addr:street (street names), addr:postcode (postal codes) and phone (phone numbers) for this project. The street name and phone numbers are a focus as businesses need this information to be accurate for customers to contact them. The postal code was picked due to some outliers being found that are not postcodes of the city area."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Auditing Adress Street Names  \n",
    "**Process**: The audit street name function from the Udacity Data Wrangling course was modified to catch street names with special characters. The returned dictionary information was reviewed and used to modify the mapping dictionary and to update the expected_list of street type endings. The [`audit_street_validate_corrections()`](modules\\audit_streets.py \"Link to containing .py file\") was modified to use the mapping dictionary to test corrections to the street names. \n",
    "\n",
    "**Findings**: The errors found during the street auditing process were:\n",
    "* Streets with missing or non-standard street type endings.\n",
    "    * Street names ending with Cv, Cv. and Cove\n",
    "    * Street names ending with \"Suite 301\" or \"United States\"\n",
    "* Non-standard formating for highways and directions.\n",
    "    * IH-35, IH35, Interstate 35, I-35, Highway 35\n",
    "    * North, N, N. and directions appearing at end of street name after street type \n",
    "* Steet names containing partial or full addresses such as\n",
    "    * house number or apartment number\n",
    "    * complete address written out with postal code, state and country\n",
    "\n",
    "<br>\n",
    "The following code are examples of street names found during auditing.\n",
    "\n",
    "\n",
    "```   {\n",
    "       ',': set(['11066 Pecan Park St  300, Cedar Park, TX',\n",
    "                 '1335 E Whitestone Blvd T100, Cedar Park, TX 78613, United States',\n",
    "                 'Louis Henna Blvd, TX 45 Frontage Road',\n",
    "                 'N Interstate Hwy 35, Round Rock, TX 78681',\n",
    "                 'S Interstate 35, #260']),\n",
    "       '35': set(['Highway Interstate 35',\n",
    "                  'N Interstate Hwy 35',\n",
    "                  'North IH 35',\n",
    "                  'North Interstate 35',\n",
    "                  'S Interstate 35',\n",
    "                  'S Interstate Highway 35',\n",
    "                  'South Interstate 35']),\n",
    "       685': set(['FM 685', 'Farm to Market 685']),\n",
    " 'Barrhead': set(['Barrhead']),\n",
    "       'Cv': set(['Copper Point Cv',\n",
    "                  'Quiet Meadows Cv',\n",
    "                  'Ripley Castle Cv',\n",
    "                  'Secluded Willow Cv']),\n",
    "      }            \n",
    "```\n",
    "To clean the street names, all corrections were added to one mapping dictionary so that specific, single instances needing cleaning would occur in the same step as the general programmatic cleaning of the street endings. This **caused a problem with partial cleanning of the street name** instead of the whole street name being cleaned. As an example, the street names \"N. IH35\" and \"IH35\" are both street names found in the data. The mapping contains these \"N. IH35\" : \"North I-35\" and also \"IH35\" : \"I-35\" to catch these instances. Since dictionaries are not ordered in Python 2.7, the N. IH35 was being caught by the \"IH35\" mapping key and converted to N. I-35 (partially cleaned) instead of \"North I-35\".\n",
    "\n",
    "To fix the issue, a separate function [`dictionary_key_length_ordered_descending()`](modules\\helper_functions.py \"Link to containing .py file\") was used to create an OrderedDict by key value length. This ensures the longer, more specific keys are found before the less specific keys. Separating out the mapping dictionaries and sending the data to both dictionaries would be another option that might reduce time and spacial complexity but would require further testing. Testing the function gave the expected results but did highlight issues with street names with valid endings but beginning with abreviation letters for directions (N, S, E, W). The cleaning function added a directions_mapping to check for this case. Examples of the cleaning function results during auditing are given below. \n",
    "\n",
    "### Sample Street Name Cleaning Results:\n",
    "```\n",
    "11066 Pecan Park Ste 300, Cedar Park, TX => Pecan Park Boulevard\n",
    "S Bell Blvd., Suite 301 => South Bell Boulevard\n",
    "U.S. 183 => US 183\n",
    "1500 S. IH35 => South I-35\n",
    "N. IH35, => North I-35\n",
    "W. Parmer Lane => West Parmer Lane\n",
    "N. IH 35 Pflugerville => North I-35\n",
    "MCNEIL RD => McNeil Road\n",
    "University Blvd => University Boulevard\n",
    "Exchange Blvd => Exchange Boulevard\n",
    "N Heatherwilde Blvd => N Heatherwilde Boulevard\n",
    "E Palm Valley Blvd => E Palm Valley Boulevard\n",
    "South Bell Blvd => South Bell Boulevard\n",
    "200 University Blvd => University Boulevard\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{',': {'11066 Pecan Park Ste 300, Cedar Park, TX',\n       '1335 E Whitestone Blvd T100, Cedar Park, TX 78613, United States',\n       'Louis Henna Blvd, TX 45 Frontage Road',\n       'N Interstate Hwy 35, Round Rock, TX 78681',\n       'S Interstate 35, #260'},\n '-': {'Ranch-to-Market Road 620', 'N I-35 Suite 298', 'North I -35'},\n '.': {'1500 S. IH35',\n       'N. IH 35 Pflugerville',\n       'N. IH35,',\n       'S Bell Blvd., Suite 301',\n       'U.S. 183',\n       'W. Parmer Lane'},\n '112': {'County Road 112'},\n '130': {'North Sh 130', 'State Highway 130'},\n '1325': {'FM 1325'},\n '137': {'County Road 137'},\n '138': {'County Road 138'},\n '1460': {'FM 1460'},\n '170': {'County Road 170'},\n '172': {'County Road 172'},\n '176': {'County Road 176'},\n '183': {'US Highway 183', 'North Hwy 183', 'North Highway 183'},\n '2': {'N FM 620 Bldg 2'},\n '35': {'Highway Interstate 35',\n        'N Interstate Hwy 35',\n        'North IH 35',\n        'North Interstate 35',\n        'S Interstate 35',\n        'S Interstate Highway 35',\n        'South Interstate 35'},\n '620': {'N RM 620',\n         'North FM 620',\n         'North Ranch Road 620',\n         'RM 620',\n         'RR 620',\n         'Ranch Road 620'},\n '685': {'FM 685', 'Farm to Market 685'},\n 'Barrhead': {'Barrhead'},\n 'Blvd': {'200 University Blvd',\n          'E Palm Valley Blvd',\n          'Ed Schmidt Blvd',\n          'Exchange Blvd',\n          'La Frontera Blvd',\n          'Louis Henna Blvd',\n          'N Heatherwilde Blvd',\n          'South Bell Blvd',\n          'University Blvd'},\n 'Blvd.': {'East Palm Valey Blvd.', 'Ed Schmidt Blvd.'},\n 'Buckskin': {'Buckskin'},\n 'Butterfly': {'Monarch Butterfly'},\n 'Canterwood': {'Canterwood'},\n 'Casitas': {'Casitas'},\n 'Castle': {'Caisteal Castle'},\n 'CastlePath': {'Raglan CastlePath'},\n 'Crossings': {'Artesian Springs Crossings'},\n 'Cv': {'Copper Point Cv',\n        'Quiet Meadows Cv',\n        'Ripley Castle Cv',\n        'Secluded Willow Cv'},\n 'Dalmahoy': {'Dalmahoy'},\n 'Dr': {'Dandridge Dr',\n        'Double Creek Dr',\n        'Forest Creek Dr',\n        'Mineral Dr',\n        'Round Rock West Dr',\n        \"St Mary's Dr\"},\n 'Dr.': {'Stonebridge Dr.'},\n 'Dublin': {'Royal Dublin'},\n 'Dust': {'Trail Dust'},\n 'East': {'Black Locust Drive East'},\n 'Edenderry': {'Edenderry'},\n 'G-145': {'E Whitestone Blvd Ste G-145'},\n 'IH-35': {'N IH-35'},\n 'Juniper': {'Brown Juniper'},\n 'Maple': {'Canyon Maple'},\n 'Marys': {'St Marys'},\n 'Meadows': {'Mellow Meadows'},\n 'N': {'Ranch Rd 620 N'},\n 'NB': {'North SH 130 NB'},\n 'North': {'Chesapeake Bay Lane North',\n           'FM 620 North',\n           'Highway 183 North',\n           'IH 35 North',\n           'Kettleman Lane North'},\n 'Pointe': {'Royal Pointe'},\n 'RD': {'MCNEIL RD'},\n 'Rd': {'Westinghouse Rd', 'Hesters Crossing Rd', 'Barley Rd'},\n 'Rd.': {'Gattis School Rd.'},\n 'Ridge': {'Murchison Ridge'},\n 'St.': {'W Front St.'},\n 'Stonebridge': {'Stonebridge'},\n 'Talamore': {'Talamore'},\n 'Trl': {'Harrier Flight Trl', 'Poppy Hills Trl'},\n 'US-79': {'US-79'},\n 'pass': {'Raven Caw pass'}}\n\nSouth Interstate 35 => South I-35\nHighway Interstate 35 => I-35\nNorth IH 35 => North I-35\nS Interstate Highway 35 => South I-35\nS Interstate 35 => South I-35\nNorth Interstate 35 => North I-35\nN Interstate Hwy 35 => North I-35\nStonebridge Dr. => Stonebridge Drive\nRound Rock West Dr => Round Rock West Drive\nDandridge Dr => Dandridge Drive\nMineral Dr => Mineral Drive\nForest Creek Dr => Forest Creek Drive\nDouble Creek Dr => Double Creek Drive\nSt Mary's Dr => St Mary's Drive\nUS-79 => US 79\nChesapeake Bay Lane North => Chesapeake Bay Lane \nFM 620 North => North Farm to Market 620\nHighway 183 North => North US 183\nIH 35 North => North I-35\nKettleman Lane North => Kettleman Lane \n1500 S. IH35 => South I-35\nS Bell Blvd., Suite 301 => South Bell Boulevard\nN. IH 35 Pflugerville => North I-35\nN. IH35, => North I-35\nU.S. 183 => US 183\nW. Parmer Lane => West Parmer Lane\nN Interstate Hwy 35, Round Rock, TX 78681 => North I-35\n11066 Pecan Park Ste 300, Cedar Park, TX => Pecan Park Boulevard\nS Interstate 35, #260 => South I-35\n1335 E Whitestone Blvd T100, Cedar Park, TX 78613, United States => East Whitestone Boulevard\nLouis Henna Blvd, TX 45 Frontage Road => Louis Henna Boulevard\nLa Frontera Blvd => La Frontera Boulevard\nN Heatherwilde Blvd => North Heatherwilde Boulevard\nEd Schmidt Blvd => Ed Schmidt Boulevard\nExchange Blvd => Exchange Boulevard\nE Palm Valley Blvd => East Palm Valley Boulevard\nUniversity Blvd => University Boulevard\n200 University Blvd => University Boulevard\nLouis Henna Blvd => Louis Henna Boulevard\nSouth Bell Blvd => South Bell Boulevard\nRanch-to-Market Road 620 => Ranch Road 620\nN I-35 Suite 298 => North I-35\nNorth I -35 => North I-35\nCasitas => Casitas Drive\nRR 620 => Ranch Road 620\nRM 620 => Ranch Road 620\nN RM 620 => North Ranch Road 620\nNorth FM 620 => North Farm to Market 620\nNorth Ranch Road 620 => North Ranch Road 620\nRanch Road 620 => Ranch Road 620\nMellow Meadows => Mellow Meadows Drive\nHarrier Flight Trl => Harrier Flight Trail\nPoppy Hills Trl => Poppy Hills Trail\nRoyal Pointe => Royal Pointe Drive\nCaisteal Castle => Caisteal Castle Path\nRipley Castle Cv => Ripley Castle Cove\nSecluded Willow Cv => Secluded Willow Cove\nCopper Point Cv => Copper Point Cove\nQuiet Meadows Cv => Quiet Meadows Cove\nBrown Juniper => Brown Juniper Way\nE Whitestone Blvd Ste G-145 => East Whitestone Boulevard\nCounty Road 172 => County Road 172\nN FM 620 Bldg 2 => North Farm to Market 620\nEast Palm Valey Blvd. => East Palm Valey Boulevard\nEd Schmidt Blvd. => Ed Schmidt Boulevard\nUS Highway 183 => US 183\nNorth Hwy 183 => North US 183\nNorth Highway 183 => North US 183\nFM 685 => Farm to Market 685\nFarm to Market 685 => Farm to Market 685\nN IH-35 => North I-35\nWestinghouse Rd => Westinghouse Road\nHesters Crossing Rd => Hesters Crossing Road\nBarley Rd => Barley Road\nFM 1325 => Farm to Market 1325\nCounty Road 170 => County Road 170\nMCNEIL RD => McNeil Road\nRoyal Dublin => Royal Dublin Drive\nEdenderry => Edenderry Drive\nBarrhead => Barrhead Cove\nDalmahoy => Dalmahoy Drive\nStonebridge => Stonebridge Drive\nBuckskin => Buckskin Drive\nTrail Dust =>  Trail Dust Drive\nSt Marys => St Mary's Drive\nRaven Caw pass => Raven Caw Pass\nRaglan CastlePath => Raglan Castle Path\nCanyon Maple => Canyon Maple Drive\nBlack Locust Drive East => Black Locust Drive \nCanterwood => Canterwood Lane\nTalamore => Talamore Road\nNorth Sh 130 => North State Highway 130\nState Highway 130 => State Highway 130\nCounty Road 138 => County Road 138\nMurchison Ridge => Murchison Ridge Trail\nGattis School Rd. => Gattis School Road\nCounty Road 137 => County Road 137\nCounty Road 176 => County Road 176\nW Front St. => West Front Street\nNorth SH 130 NB => North State Highway 130\nRanch Rd 620 N => North Ranch Road 620\nMonarch Butterfly => Monarch Butterfly Way\nCounty Road 112 => County Road 112\nFM 1460 => Farm to Market 1460\nArtesian Springs Crossings => Artesian Springs Crossing\n\nTotal Changes: 105\n\nThe following street names were identified but no changes were made \n\n[('North Ranch Road 620', 'North Ranch Road 620'),\n ('Ranch Road 620', 'Ranch Road 620'),\n ('County Road 172', 'County Road 172'),\n ('Farm to Market 685', 'Farm to Market 685'),\n ('County Road 170', 'County Road 170'),\n ('State Highway 130', 'State Highway 130'),\n ('County Road 138', 'County Road 138'),\n ('County Road 137', 'County Road 137'),\n ('County Road 176', 'County Road 176'),\n ('County Road 112', 'County Road 112')]\n"
     ]
    }
   ],
   "source": [
    "from modules.audit_streets import audit_street_validate_corrections\n",
    "\n",
    "OSM_FILE = \"data\\\\round_rock.xml\"\n",
    "\n",
    "audit_street_validate_corrections(OSM_FILE)"
   ]
  },
  {
   "source": [
    "## Auditing Postal Codes\n",
    "The postal codes were audited next using the [`audit_postacode()`](modules\\audit_postcodes.py \"Link to containing .py file\") function. Post codes in the United States are comprised of 5 digits with the first digit giving the National Area, the next two digits giving a Sectional (Regional) Area and the last two digits giving the Delivery Area. Therefore all postcodes should be 5 digits. To validate the data, postcodes for the City of Round Rock were placed in a list to compare postcode values to. The results of the audit showed that all postal codes for the city were found in the data file. There were also 10 postal codes that were not listed as belonging to Round Rock. The results of the audit is shown below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nThere was at least one instance of each Round Rock postcode found in the data.\n\n\nThe following postcodes are not identified as Round Rock postcodes and need review.\n\n{'78729', '78758', '78682', '78680', '78750', '78641', '78613', '787664', '78728-1275', '78621'}\n\n\n787664 cleaned to --> 78664\n78728-1275 cleaned to --> 78728\n78621 cleaned to --> 78681\n\n\n\nCompleted\n"
     ]
    }
   ],
   "source": [
    "from modules.audit_postcodes import audit_postalcode\n",
    "\n",
    "audit_postalcode(OSM_FILE = \"data\\\\round_rock.xml\")\n",
    "print(\"\\nCompleted\")"
   ]
  },
  {
   "source": [
    "### Validating the postcode data\n",
    "Using [Round Rock's City GIS software](https://maps.roundrocktexas.gov/cityview/),  researching and exploring specific elements, the following information was discovered.\n",
    "* The postcodes 78613, 78758, 78729, 78641 and 78750 are neighborhoods at the border of the city\n",
    "* The postcode 78621 does not seem to be touching the border of Round Rock and will require further investigation.\n",
    "* The postcodes 78680 and 78682 are listed as specially designated zoning regions used for PO Box, high volume or historical significance postcodes.\n",
    "* The postcode 787664 is a typo and should be 78664. This will be verified using the elements street, longitude and latitude data.\n",
    "* The postcode 78728-1275 has the expanded code added (ZIP+4). The expanded code will be removed during cleaning."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Running the [`explore_postcode_details()`](modules\\audit_postcodes.py \"Links to containing .py file\") function on the two zip codes showed both street addresses are in Round Rock and will be corrected using a mapping dictionary during cleaning.\n",
    "* The 78621 postcode (Hoody's) should be 78681 and is likely a typo (2 instead of 8 on keypad).\n",
    "* The 787664 postcode (Self Storage Facility) is a typo and should be 78664."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Audit Phone Numbers\n",
    "Auditing the phone numbers using the [`audit_phones()`](modules\\audit_phones.py \"Link to containing .py file\") function shows mostly conisitent data with variations between using dashes, spaces and parenthesis to delimenate the numbers. Other issues that were discovered are:\n",
    "* Numbers using more that one deliminator such as spaces and parenthesis\n",
    "* Numbers using no deliminators\n",
    "* Unicode character code /2100 found intead of '-'\n",
    "* Four numbers where found more than once in the data\n",
    "\n",
    "The results of the phone audit are below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nAll phone numbers were checked.\n\nThere are 45 malformed numbers, 4 duplicate numbers and 245 wellformed numbers.\n\nThe malformed numbers found are: \n\n{'(512) 246-7941': 'fix_number',\n '+1 (512) 469-7000': 'fix_number',\n '+1 (512) 759-5900': 'fix_number',\n '+1 512 218 5062': 'fix_number',\n '+1 512 218 9888': 'fix_number',\n '+1 512 238 0820': 'fix_number',\n '+1 512 244 3737': 'fix_number',\n '+1 512 248 7000': 'fix_number',\n '+1 512 252 1133': 'fix_number',\n '+1 512 255 7000': 'fix_number',\n '+1 512 255 7530': 'fix_number',\n '+1 512 258 8114': 'fix_number',\n '+1 512 277 6959': 'fix_number',\n '+1 512 310 7600': 'fix_number',\n '+1 512 310 7678': 'fix_number',\n '+1 512 324 4000': 'fix_number',\n '+1 512 341 1000': 'fix_number',\n '+1 512 362 9525': 'fix_number',\n '+1 512 402 7811': 'fix_number',\n '+1 512 528 7000': 'fix_number',\n '+1 512 532 2200': 'fix_number',\n '+1 512 600 0145': 'fix_number',\n '+1 512 637 6890': 'fix_number',\n '+1 512 733 9660': 'fix_number',\n '+1 512 990 5413': 'fix_number',\n '+1 512)351 3179': 'fix_number',\n '+1 512-244-8500': 'fix_number',\n '+1 512-260-5443': 'fix_number',\n '+1 512-260-6363': 'fix_number',\n '+1 512-310-8952': 'fix_number',\n '+1 512-338-8805': 'fix_number',\n '+1 512-341-7387': 'fix_number',\n '+1 512-421-5911': 'fix_number',\n '+1 512-535-5160': 'fix_number',\n '+1 512-535-6317': 'fix_number',\n '+1 512-733-6767': 'fix_number',\n '+1 512-851-8777': 'fix_number',\n '+1 737 757 3100': 'fix_number',\n '+1-737-484‑0700': 'fix_number',\n '+1512-413-9671': 'fix_number',\n '+1512-909-2528': 'fix_number',\n '+15123885728': 'fix_number',\n '+15124282300': 'fix_number',\n '+15124648382': 'fix_number',\n '1+512-696-5209': 'fix_number'}\n\nThe 4 duplicated items are: \n\n{'+1-512-310-8791', '+1-512-428-2500', '+1-512-238-0475', '+1-512-336-1328'}\n\n Note: The returned malformed number dictionary can be run through \n\tmodules.helper_functions phone_partial_clean(phone_dict) \n\tfunction to partially clean the numbers given in the key.\n\n"
     ]
    }
   ],
   "source": [
    "from modules.audit_phones import audit_phones\n",
    "\n",
    "OSM_FILE = \"data\\\\round_rock.xml\"\n",
    "# OSM_FILE = \"data\\\\round_rock_sample.xml\"\n",
    "# print_number_type = ['duplicate', 'malformed']\n",
    "\n",
    "phone_numbers_to_clean = audit_phones(OSM_FILE)"
   ]
  },
  {
   "source": [
    "### Cleaning the Phone Numbers\n",
    "To make the data more consistent, it was decided to follow the US phone pattern with leading country code separated by dashes ( +1-###-###-#### ). The numbers that did not follow the US phone pattern were added as a key to the malformed dictionary. A partial clean was first performed using the [`phone_partial_clean()`](modules\\helper_functions.py \"Link to conataining .py file\") function. This function programatically cleans the data by using the `replace()` string method calls to replace:\n",
    "\n",
    "* All blanks with hyphens,\n",
    "* Removes all parenthesis\n",
    "\n",
    "Of the 45 original phone number corrections, the new dictionary still contained 8 phone numbers with errors (see below). These could be programatically cleaned using specific algorithms for each. The time complexity would increase significantly for so few corrections needed. Therefore, it was decided to manually edit these numbers in the mapping dictionary.\n",
    "\n",
    "Numbers not fixed by partial clean (manually fixed):\n",
    "\n",
    "```\n",
    " '(512) 246-7941'       : '512-246-7941',\n",
    " '+1 512)351 3179'      : '+1-512351-3179',\n",
    " u'+1-737-484\\u20110700': u'+1-737-484\\u20110700',\n",
    " '+1512-413-9671'       : '+1512-413-9671',\n",
    " '+1512-909-2528'       : '+1512-909-2528',\n",
    " '+15123885728'         : '+15123885728',\n",
    " '+15124282300'         : '+15124282300',\n",
    " '+15124648382'         : '+15124648382',\n",
    " \n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'(512) 246-7941': '512-246-7941',\n '+1 (512) 469-7000': '+1-512-469-7000',\n '+1 (512) 759-5900': '+1-512-759-5900',\n '+1 512 218 5062': '+1-512-218-5062',\n '+1 512 218 9888': '+1-512-218-9888',\n '+1 512 238 0820': '+1-512-238-0820',\n '+1 512 244 3737': '+1-512-244-3737',\n '+1 512 248 7000': '+1-512-248-7000',\n '+1 512 252 1133': '+1-512-252-1133',\n '+1 512 255 7000': '+1-512-255-7000',\n '+1 512 255 7530': '+1-512-255-7530',\n '+1 512 258 8114': '+1-512-258-8114',\n '+1 512 277 6959': '+1-512-277-6959',\n '+1 512 310 7600': '+1-512-310-7600',\n '+1 512 310 7678': '+1-512-310-7678',\n '+1 512 324 4000': '+1-512-324-4000',\n '+1 512 341 1000': '+1-512-341-1000',\n '+1 512 362 9525': '+1-512-362-9525',\n '+1 512 402 7811': '+1-512-402-7811',\n '+1 512 528 7000': '+1-512-528-7000',\n '+1 512 532 2200': '+1-512-532-2200',\n '+1 512 600 0145': '+1-512-600-0145',\n '+1 512 637 6890': '+1-512-637-6890',\n '+1 512 733 9660': '+1-512-733-9660',\n '+1 512 990 5413': '+1-512-990-5413',\n '+1 512)351 3179': '+1-512351-3179',\n '+1 512-244-8500': '+1-512-244-8500',\n '+1 512-260-5443': '+1-512-260-5443',\n '+1 512-260-6363': '+1-512-260-6363',\n '+1 512-310-8952': '+1-512-310-8952',\n '+1 512-338-8805': '+1-512-338-8805',\n '+1 512-341-7387': '+1-512-341-7387',\n '+1 512-421-5911': '+1-512-421-5911',\n '+1 512-535-5160': '+1-512-535-5160',\n '+1 512-535-6317': '+1-512-535-6317',\n '+1 512-733-6767': '+1-512-733-6767',\n '+1 512-851-8777': '+1-512-851-8777',\n '+1 737 757 3100': '+1-737-757-3100',\n '+1-737-484‑0700': '+1-737-484‑0700',\n '+1512-413-9671': '+1512-413-9671',\n '+1512-909-2528': '+1512-909-2528',\n '+15123885728': '+15123885728',\n '+15124282300': '+15124282300',\n '+15124648382': '+15124648382',\n '1+512-696-5209': '1+512-696-5209'}\n"
     ]
    }
   ],
   "source": [
    "from modules.helper_functions import phone_partial_clean\n",
    "\n",
    "phones = {'(512) 246-7941': 'fix_number',\n",
    "    '+1 (512) 469-7000': 'fix_number',\n",
    "    '+1 (512) 759-5900': 'fix_number',\n",
    "    '+1 512 218 5062': 'fix_number',\n",
    "    '+1 512 218 9888': 'fix_number',\n",
    "    '+1 512 238 0820': 'fix_number',\n",
    "    '+1 512 244 3737': 'fix_number',\n",
    "    '+1 512 248 7000': 'fix_number',\n",
    "    '+1 512 252 1133': 'fix_number',\n",
    "    '+1 512 255 7000': 'fix_number',\n",
    "    '+1 512 255 7530': 'fix_number',\n",
    "    '+1 512 258 8114': 'fix_number',\n",
    "    '+1 512 277 6959': 'fix_number',\n",
    "    '+1 512 310 7600': 'fix_number',\n",
    "    '+1 512 310 7678': 'fix_number',\n",
    "    '+1 512 324 4000': 'fix_number',\n",
    "    '+1 512 341 1000': 'fix_number',\n",
    "    '+1 512 362 9525': 'fix_number',\n",
    "    '+1 512 402 7811': 'fix_number',\n",
    "    '+1 512 528 7000': 'fix_number',\n",
    "    '+1 512 532 2200': 'fix_number',\n",
    "    '+1 512 600 0145': 'fix_number',\n",
    "    '+1 512 637 6890': 'fix_number',\n",
    "    '+1 512 733 9660': 'fix_number',\n",
    "    '+1 512 990 5413': 'fix_number',\n",
    "    '+1 512)351 3179': 'fix_number',\n",
    "    '+1 512-244-8500': 'fix_number',\n",
    "    '+1 512-260-5443': 'fix_number',\n",
    "    '+1 512-260-6363': 'fix_number',\n",
    "    '+1 512-310-8952': 'fix_number',\n",
    "    '+1 512-338-8805': 'fix_number',\n",
    "    '+1 512-341-7387': 'fix_number',\n",
    "    '+1 512-421-5911': 'fix_number',\n",
    "    '+1 512-535-5160': 'fix_number',\n",
    "    '+1 512-535-6317': 'fix_number',\n",
    "    '+1 512-733-6767': 'fix_number',\n",
    "    '+1 512-851-8777': 'fix_number',\n",
    "    '+1 737 757 3100': 'fix_number',\n",
    "    u'+1-737-484\\u20110700': 'fix_number',\n",
    "    '+1512-413-9671': 'fix_number',\n",
    "    '+1512-909-2528': 'fix_number',\n",
    "    '+15123885728': 'fix_number',\n",
    "    '+15124282300': 'fix_number',\n",
    "    '+15124648382': 'fix_number',\n",
    "    '1+512-696-5209': 'fix_number'}\n",
    "\n",
    "new_phone_mapping = phone_partial_clean(phones)"
   ]
  },
  {
   "source": [
    "## Creating CSV Files and Database Import\n",
    "With all the planned cleaning verified, the data was converted to csv files using the [`xml_to_csv()`](data.py \"Link to containing .py file.\") function in the data.py file. To identify attributes to be cleaned, the Tag element attributes are checked to see if they are in the update_list. If the attribute key is one of the attributes to be cleaned, they are sent to the [`update_values()`](modules\\update_values.py \"Link to containing .py file.\") function which acts as flow control and sends the value to the appropriate cleaning function returning the cleaned value.\n",
    "\n",
    "``` python\n",
    "...\n",
    "UPDATE_LIST = ['addr:street', 'addr:postcode', 'phone']\n",
    "...\n",
    "    for child in element.iter('tag'):\n",
    "        tag_dict = {}\n",
    "\n",
    "        if child.attrib['k'] in UPDATE_LIST:                                        \n",
    "            child.attrib['v'] = update_value(child.attrib['k'], child.attrib['v'])\n",
    "...\n",
    "```\n",
    "\n",
    "During the conversion to csv, the **entire dataset was validated using the cerberus package** and the [schema.py](schema.py \"Link to .py file\") file. Additionally, after the conversion to csv, the csv files were descriminately audited using filters to confirm corrections.\n",
    "\n",
    "**Note**: Due to the time complexity of running the cerberus validation, smaller test.xml and round_rock_sample.xml files were run prior to validating the entire data set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Started\n",
      "**** Completed. Check sql/csv folder for csv files. ****\n"
     ]
    }
   ],
   "source": [
    "# Validate = True was run on the entire dataset and passed. The runtime was over 1 hour.\n",
    "# Then, validate was set to False so that it will not be run by mistake.\n",
    "\n",
    "from data import xml_to_csv\n",
    "\n",
    "OSM_FILE = \"data\\\\round_rock.xml\"               # Full file (~220 MB)\n",
    "# OSM_FILE = \"data\\\\round_rock_sample.xml\"      # Sample file (~10 MB)\n",
    "# OSM_FILE = \"data\\\\test.xml\"                   # Test file for testing functionality and bugs (126 KB)\n",
    "\n",
    "xml_to_csv(OSM_FILE, validate=False)"
   ]
  },
  {
   "source": [
    "## SQL Import\n",
    "After converting to csv files, the data was then loaded into an Sqlite3 database, [Round_RockDb.db](sql \"Link opens containing folder\"), and the function [`process_sql()`](modules\\process_sql.py \"Link Opens containing .py file\") was used to create the tables for the database. Included in the [`process_sql()`](modules\\process_sql.py \"Link Opens containing .py file\") function, is the table schema used to create the tables. The table schema is also available in the sql folder in the [create_table_schema](sql\\create_table_schema.py \"Link opens containing file\") file for convenience.  \n",
    "\n",
    "Finally, the csv files were loaded into the tables using the following code and the [`csv_to_sql()`](modules\\process_sql.py \"Link opens containing .py file.\") function.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "``` python\n",
    "    # import csv to sqlite db tables: nodes_tags, nodes, ways_nodes, ways_tags, ways\n",
    "\n",
    "    from modules.process_sql import csv_to_sql\n",
    "\n",
    "    # Dictionary of file paths and table names\n",
    "    csv_file_table ={'sql\\\\csv\\\\nodes_tags.csv' : 'nodes_tags', \n",
    "                     'sql\\\\csv\\\\nodes.csv' : 'nodes', \n",
    "                     'sql\\\\csv\\\\ways_nodes.csv' : 'ways_nodes', \n",
    "                     'sql\\\\csv\\\\ways_tags.csv' : 'ways_tags', \n",
    "                     'sql\\\\csv\\\\ways.csv' : 'ways'\n",
    "                    }\n",
    "\n",
    "    db_file = 'sql\\\\Round_RockDb.db'\n",
    "\n",
    "    for csv_file, db_table in csv_file_table.items():\n",
    "        csv_to_sql(csv_file, db_file, db_table)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tables = [\"\"\"DROP TABLE IF EXISTS nodes;\"\"\",\n",
    "                \n",
    "                \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS nodes (\n",
    "                    id INTEGER PRIMARY KEY,\n",
    "                    lat REAL NOT NULL,\n",
    "                    lon REAL NOT NULL,\n",
    "                    user TEXT NOT NULL,\n",
    "                    uid INTEGER NOT NULL,\n",
    "                    version TEXT NOT NULL,\n",
    "                    changeset INTEGER NOT NULL,\n",
    "                    timestamp DATE NOT NULL\n",
    "                );\"\"\",\n",
    "\n",
    "                \"\"\"DROP TABLE IF EXISTS nodes_tags;\"\"\",\n",
    "\n",
    "                \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS nodes_tags (\n",
    "                    id INTEGER NOT NULL,\n",
    "                    key TEXT NOT NULL,\n",
    "                    value TEXT NOT NULL,\n",
    "                    type TEXT NOT NULL,\n",
    "                    FOREIGN KEY (id) REFERENCES nodes (id),\n",
    "                    FOREIGN KEY (id) REFERENCES ways_nodes (node_id)\n",
    "                );\"\"\",\n",
    "\n",
    "                \"\"\"DROP TABLE IF EXISTS ways;\"\"\",\n",
    "\n",
    "                \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS ways (\n",
    "                    id INTEGER PRIMARY KEY,\n",
    "                    user TEXT NOT NULL,\n",
    "                    uid INTEGER NOT NULL,\n",
    "                    version TEXT NOT NULL,\n",
    "                    changeset INTEGER NOT NULL,\n",
    "                    timestamp DATE NOT NULL\n",
    "                );\"\"\",\n",
    "\n",
    "                \"\"\"DROP TABLE IF EXISTS ways_nodes;\"\"\",\n",
    "\n",
    "                \"\"\"               \n",
    "                CREATE TABLE IF NOT EXISTS ways_nodes (\n",
    "                    id INTEGER NOT NULL,\n",
    "                    node_id INTEGER NOT NULL,\n",
    "                    position INTEGER NOT NULL,\n",
    "                    FOREIGN KEY (id) REFERENCES ways (id),\n",
    "                    FOREIGN KEY (node_id) REFERENCES nodes (id)\n",
    "                );\"\"\",\n",
    "\n",
    "                \"\"\"DROP TABLE IF EXISTS ways_tags;\"\"\",\n",
    "                \n",
    "                \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS ways_tags (\n",
    "                    id INTEGER NOT NULL,\n",
    "                    key TEXT NOT NULL,\n",
    "                    value TEXT NOT NULL,\n",
    "                    type TEXT NOT NULL,\n",
    "                    FOREIGN KEY (id) REFERENCES ways (id),\n",
    "                    FOREIGN KEY (id) REFERENCES ways_nodes (node_id)\n",
    "                );\"\"\"\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Db Open:  SQLite3 version 2.6.0\n",
      "Closing Db connection\n",
      "Connection closed.\n",
      "\n",
      "Db Open:  SQLite3 version 2.6.0\n",
      "Closing Db connection\n",
      "Connection closed.\n",
      "\n",
      "Db Open:  SQLite3 version 2.6.0\n",
      "Closing Db connection\n",
      "Connection closed.\n",
      "\n",
      "Db Open:  SQLite3 version 2.6.0\n",
      "Closing Db connection\n",
      "Connection closed.\n",
      "\n",
      "Db Open:  SQLite3 version 2.6.0\n",
      "Closing Db connection\n",
      "Connection closed.\n",
      "\n",
      "Db Open:  SQLite3 version 2.6.0\n",
      "Closing Db connection\n",
      "Connection closed.\n",
      "\n",
      "Db Open:  SQLite3 version 2.6.0\n",
      "Closing Db connection\n",
      "Connection closed.\n",
      "\n",
      "Db Open:  SQLite3 version 2.6.0\n",
      "Closing Db connection\n",
      "Connection closed.\n",
      "\n",
      "Db Open:  SQLite3 version 2.6.0\n",
      "Closing Db connection\n",
      "Connection closed.\n",
      "\n",
      "Db Open:  SQLite3 version 2.6.0\n",
      "Closing Db connection\n",
      "Connection closed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from modules.process_sql import process_sql, create_tables\n",
    "\n",
    "db_file = 'sql\\\\Round_RockDb.db'\n",
    "\n",
    "for create_table in create_tables:\n",
    "    process_sql(db_file, create_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Processing sql\\csv\\nodes_tags.csv file to nodes_tags table.\n",
      "Closing Db connection\n",
      "Connection closed.\n",
      "\n",
      "\n",
      "Processing sql\\csv\\nodes.csv file to nodes table.\n",
      "Closing Db connection\n",
      "Connection closed.\n",
      "\n",
      "\n",
      "Processing sql\\csv\\ways_nodes.csv file to ways_nodes table.\n",
      "Closing Db connection\n",
      "Connection closed.\n",
      "\n",
      "\n",
      "Processing sql\\csv\\ways_tags.csv file to ways_tags table.\n",
      "Closing Db connection\n",
      "Connection closed.\n",
      "\n",
      "\n",
      "Processing sql\\csv\\ways.csv file to ways table.\n",
      "Closing Db connection\n",
      "Connection closed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import csv to sqlite db tables: nodes_tags, nodes, ways_nodes, ways_tags, ways\n",
    "\n",
    "from modules.process_sql import csv_to_sql\n",
    "\n",
    "csv_file_table ={'sql\\\\csv\\\\nodes_tags.csv' : 'nodes_tags', \n",
    "                 'sql\\\\csv\\\\nodes.csv' : 'nodes', \n",
    "                 'sql\\\\csv\\\\ways_nodes.csv' : 'ways_nodes', \n",
    "                 'sql\\\\csv\\\\ways_tags.csv' : 'ways_tags', \n",
    "                 'sql\\\\csv\\\\ways.csv' : 'ways'\n",
    "                 }\n",
    "\n",
    "db_file = 'sql\\\\Round_RockDb.db'\n",
    "\n",
    "for csv_file, db_table in csv_file_table.items():\n",
    "    csv_to_sql(csv_file, db_file, db_table)"
   ]
  },
  {
   "source": [
    "### Database Queries: Statistical Overview of Dataset\n",
    "The following sections contain information about the Size of the files compared to the database, number of unique users, number of nodes and ways, and number of:\n",
    "* unique street names\n",
    "* unique postcodes and \n",
    "* unique phone numbers\n",
    "\n",
    "In addition, I will look at the occurrence counts in the data and the top 10 in the street names and postcode groups to find which are most represented in the data. I will also examine the phone numbers to determine which numbers appeared more than once in the data. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Size of Files\n",
    "The [`get_file_info()`](modules\\helper_functions.py \"Link to containing .py file\") helper function was used to gather information about specific files used. The original data was 220 MB in size and the database has reduces this size by 101 MB to 119 MB. The total of all the csv files are 135 MB.\n",
    "\n",
    "\n",
    "```\n",
    "    round_rock.xml...............220 MB\n",
    "    round_rockdb.db..............119 MB\n",
    "    nodes_tags.csv.................1 MB\n",
    "    nodes.csv.....................92 MB\n",
    "    ways_nodes.csv................26 MB\n",
    "    ways_tags.csv..................9 MB\n",
    "    ways.csv.......................7 MB\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "round_rock.xml...............220 MB\nround_rockdb.db..............119 MB\nnodes_tags.csv.................1 MB\nnodes.csv.....................92 MB\nways_nodes.csv................26 MB\nways_tags.csv..................9 MB\nways.csv.......................7 MB\n"
     ]
    }
   ],
   "source": [
    "from modules.helper_functions import get_file_info\n",
    "\n",
    "dw_file_paths = ['data/round_rock.xml', \n",
    "                 'sql/round_rockdb.db', \n",
    "                 'sql/csv/nodes_tags.csv', \n",
    "                 'sql/csv/nodes.csv', \n",
    "                 'sql/csv/ways_nodes.csv', \n",
    "                 'sql/csv/ways_tags.csv', \n",
    "                 'sql/csv/ways.csv'\n",
    "                 ]\n",
    "\n",
    "get_file_info(dw_file_paths)\n"
   ]
  },
  {
   "source": [
    "### Number of Unique Users\n",
    "\n",
    "``` sql\n",
    "    SELECT COUNT(DISTINCT (wnUnion.uid)) as \"Total Unique Users\" \n",
    "    FROM (SELECT uid FROM ways UNION ALL SELECT uid FROM nodes) as wnUnion;\n",
    "```\n",
    "\n",
    "Total Unique Users: **1054**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Total Unique Users\n0                1054\n"
     ]
    }
   ],
   "source": [
    "from modules.process_sql import sql_query\n",
    "\n",
    "db_file = 'sql\\\\Round_RockDb.db'\n",
    "\n",
    "sql_statement = \"\"\"SELECT COUNT(DISTINCT (wnUnion.uid)) as \"Total Unique Users\" \n",
    "                   FROM (SELECT uid FROM ways UNION ALL SELECT uid FROM nodes) as wnUnion;\"\"\"\n",
    "\n",
    "sql_query(db_file, sql_statement)"
   ]
  },
  {
   "source": [
    "### Top 10 Users Represented in Data\n",
    "The top 10 users are listed below. The data shows at least one user likely using two user ids and accounting for the top 2 appearances. The suffix \"*_atxbuildings*\" was found to be from an [openstreetmap guide](https://wiki.openstreetmap.org/wiki/Austin,_TX/Buildings_Import/Software_Setup) on an Austin, TX wiki page.\n",
    "<br>\n",
    "\n",
    "``` sql\n",
    "    SELECT user, uid, COUNT(wnUnion.uid) as \"Appearance Count\" \n",
    "                   FROM (SELECT user, uid FROM ways \n",
    "                         UNION ALL \n",
    "                         SELECT user, uid FROM nodes) as wnUnion\n",
    "                   GROUP BY user, uid\n",
    "                   ORDER BY \"Appearance Count\" DESC\n",
    "                   LIMIT 10;\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "|           user             |   |     uid     |   |  Appearance Count  |\n",
    "|        ---------:          | - |  ---------: | - | -----------------: |\n",
    "|  ccjjmartin__atxbuildings  |   |  3405475    |   |      312213        |\n",
    "|   ccjjmartin_atxbuildings  |   |  3370181    |   |      309281        |\n",
    "|    patisilva_atxbuildings  |   |  3369502    |   |      122028        |\n",
    "|            SathyaPendyala  |   |  3618405    |   |       30931        |\n",
    "|                     s0707  |   | 11358207    |   |       25687        |\n",
    "|           woodpeck_fixbot  |   |   147510    |   |       24508        |\n",
    "|       wilsaj_atxbuildings  |   |  3341346    |   |       24321        |\n",
    "|                technogeek  |   |    98830    |   |       18930        |\n",
    "|                    torapa  |   |  5446055    |   |       11552        |\n",
    "|                   JanineG  |   | 12179240    |   |       11016        |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "The \"**_atxbuildings** reference was found in 5 users including the duplicated user cjmartin. Also, checking for other user names with \"cjmartin\" showed no other users with that name in their user name.  \n",
    "<br>\n",
    "\n",
    "\n",
    "``` sql\n",
    "    SELECT user, uid, COUNT(wnUnion.uid) as \"Appearance Count\" \n",
    "                   FROM (SELECT user, uid FROM ways \n",
    "                         UNION ALL \n",
    "                         SELECT user, uid FROM nodes) as wnUnion\n",
    "                   WHERE user LIKE '%atxbuildings%'\n",
    "                   GROUP BY user, uid\n",
    "                   ORDER BY \"Appearance Count\" DESC;\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "|           user             |   |     uid     |   |  Appearance Count  |\n",
    "|        ---------:          | - |  ---------: | - | -----------------: |\n",
    "|  ccjjmartin__atxbuildings  |   |  3405475    |   |      312213        |\n",
    "|   ccjjmartin_atxbuildings  |   |  3370181    |   |      309281        |\n",
    "|    patisilva_atxbuildings  |   |  3369502    |   |      122028        |\n",
    "|       wilsaj_atxbuildings  |   |  3341346    |   |       24321        |\n",
    "|  lyzidiamond_atxbuildings  |   |  3409435    |   |        2292        |\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                       user       uid  Appearance Count\n0  ccjjmartin__atxbuildings   3405475            312213\n1   ccjjmartin_atxbuildings   3370181            309281\n2    patisilva_atxbuildings   3369502            122028\n3            SathyaPendyala   3618405             30931\n4                     s0707  11358207             25687\n5           woodpeck_fixbot    147510             24508\n6       wilsaj_atxbuildings   3341346             24321\n7                technogeek     98830             18930\n8                    torapa   5446055             11552\n9                   JanineG  12179240             11016\n"
     ]
    }
   ],
   "source": [
    "from modules.process_sql import sql_query\n",
    "\n",
    "db_file = 'sql\\\\Round_RockDb.db'\n",
    "\n",
    "sql_statement = \"\"\"SELECT user, uid, COUNT(wnUnion.uid) as \"Appearance Count\" \n",
    "                   FROM (SELECT user, uid FROM ways UNION ALL SELECT user, uid FROM nodes) as wnUnion\n",
    "                   GROUP BY user, uid\n",
    "                   ORDER BY \"Appearance Count\" DESC\n",
    "                   LIMIT 10;\"\"\"\n",
    "\n",
    "sql_query(db_file, sql_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                       user      uid  Appearance Count\n0  ccjjmartin__atxbuildings  3405475            312213\n1   ccjjmartin_atxbuildings  3370181            309281\n2    patisilva_atxbuildings  3369502            122028\n3       wilsaj_atxbuildings  3341346             24321\n4  lyzidiamond_atxbuildings  3409435              2292\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from modules.process_sql import sql_query\n",
    "\n",
    "db_file = 'sql\\\\Round_RockDb.db'\n",
    "\n",
    "sql_statement = r\"\"\"SELECT user, uid, COUNT(wnUnion.uid) as \"Appearance Count\" \n",
    "                   FROM (SELECT user, uid FROM ways UNION ALL SELECT user, uid FROM nodes) as wnUnion\n",
    "                   WHERE user LIKE '%atxbuildings%'\n",
    "                   GROUP BY user, uid\n",
    "                   ORDER BY \"Appearance Count\" DESC;\"\"\"\n",
    "\n",
    "sql_query(db_file, sql_statement)"
   ]
  },
  {
   "source": [
    "### Number of Nodes and Ways\n",
    "\n",
    "The **nodes** table has **1027359 rows** and the **ways** table has **112084 rows**. \n",
    "\n",
    "```\n",
    "    SELECT count(*) as \"Nodes Count\" FROM nodes;\n",
    "    SELECT count(*) as \"Ways Count\" FROM ways;\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The nodes table has 1027359 rows. \n\nThe ways table has 112084 rows. \n\n"
     ]
    }
   ],
   "source": [
    "from modules.process_sql import sql_query\n",
    "\n",
    "db_file = 'sql\\\\Round_RockDb.db'\n",
    "\n",
    "sql_statement_list = [\"\"\"SELECT count(*) as \"Nodes Count\" FROM nodes;\"\"\", \"\"\"SELECT count(*) as \"Ways Count\" FROM ways;\"\"\"]\n",
    "\n",
    "for sql_statement in sql_statement_list:\n",
    "    table_name = sql_statement.rstrip(';\"\\' ').split(' ')[-1]\n",
    "    counts_obj = sql_query(db_file, sql_statement, False)\n",
    "    for item in counts_obj:\n",
    "        counts = counts_obj[item][0]\n",
    "    \n",
    "    print(\"The {} table has {} rows. \\n\".format(table_name, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The nodes table has 1027359 rows. \n\nThe ways table has 112084 rows. \n\n"
     ]
    }
   ],
   "source": [
    "from modules.helper_functions import table_row_counts\n",
    "\n",
    "db_file = 'sql\\\\Round_RockDb.db'\n",
    "\n",
    "sql_statement_list = ['SELECT count(*) as \"Nodes Count\" FROM nodes;', 'SELECT count(*) as \"Ways Count\" FROM ways;']\n",
    "\n",
    "table_row_counts(db_file, sql_statement_list)"
   ]
  },
  {
   "source": [
    "### Number of Streets, Postcodes and Phone Numbers  \n",
    "The results of streets, postcodes and phones in the data are:\n",
    "\n",
    "|     keys  |      |   Count   |\n",
    "|    ----:  | ---  |  -------: |\n",
    "|   street  |      |   28533   |\n",
    "| postcode  |      |    7110   |\n",
    "|    phone  |      |     308   |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "``` sql\n",
    "SELECT key, COUNT(wnUnion.value) as \"Appearance Count\" \n",
    "                    FROM (SELECT key, value, type FROM ways_tags \n",
    "                          UNION ALL \n",
    "                          SELECT key, value, type FROM nodes_tags) as wnUnion\n",
    "                    WHERE (type = 'addr' AND key IN ('street', 'postcode')) OR\n",
    "                                 (type = 'regular' AND key = 'phone')\n",
    "                    GROUP BY key\n",
    "                    ORDER BY \"Appearance Count\" DESC;\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "        key  Appearance Count\n0    street             28533\n1  postcode              7110\n2     phone               308\n"
     ]
    }
   ],
   "source": [
    "from modules.process_sql import sql_query\n",
    "\n",
    "db_file = 'sql\\\\Round_RockDb.db'\n",
    "\n",
    "sql_statement = r\"\"\"SELECT key, COUNT(wnUnion.value) as \"Appearance Count\" \n",
    "                    FROM (SELECT key, value, type FROM ways_tags \n",
    "                          UNION ALL \n",
    "                          SELECT key, value, type FROM nodes_tags) as wnUnion\n",
    "                    WHERE (type = 'addr' AND key IN ('street', 'postcode')) OR\n",
    "                                 (type = 'regular' AND key = 'phone')\n",
    "                    GROUP BY key\n",
    "                    ORDER BY \"Appearance Count\" DESC;\"\"\"\n",
    "\n",
    "sql_query(db_file, sql_statement)"
   ]
  },
  {
   "source": [
    "### Top 10 Streets by Occurence in the Data\n",
    "The top 10 streets by occurence are:  \n",
    "\n",
    "**value**|    |**Unique Street Count**\n",
    ":----- | --- |:-----:\n",
    "Winding Shore Lane|   |142\n",
    "East Whitestone Boulevard|   |135\n",
    "Pencil Cactus Drive|   |131\n",
    "Dashwood Creek Drive|   |127\n",
    "Brushy Creek Road|   |117\n",
    "Derby Day Avenue|   |116\n",
    "Dorman Drive|   |110\n",
    "Loch Linnhe Loop|   |109\n",
    "Tortoise Street|   |106\n",
    "Farm Pond Lane|   |104\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "```  sql\n",
    "    SELECT value, COUNT(wnUnion.value) as \"Unique Street Count\" \n",
    "                        FROM (SELECT key, value, type FROM ways_tags \n",
    "                            UNION ALL \n",
    "                            SELECT key, value, type FROM nodes_tags) as wnUnion\n",
    "                        WHERE key = 'street' AND type = 'addr'\n",
    "                        GROUP BY value\n",
    "                        ORDER BY \"Unique Street Count\" DESC\n",
    "                        Limit 10;\n",
    "\n",
    "```\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Of the top 10 streets only **East Whitestone Blvd**, **Brushy Creek Road**, **Dorman Drive** and **Lock Linnhe Loop** are considered to be in the actual Round Rock city limits. The remaining 7 are in the map selection bounding box but are not officially in Round Rock's city limits. All streets except East Whitestone Blvd and Brushy Creek Road are residential roads. Based on this preliminary data, Round Rock businesses do not seem to be well represented in the data.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                       value  Unique Street Count\n0         Winding Shore Lane                  142\n1  East Whitestone Boulevard                  135\n2        Pencil Cactus Drive                  131\n3       Dashwood Creek Drive                  127\n4          Brushy Creek Road                  117\n5           Derby Day Avenue                  116\n6               Dorman Drive                  110\n7           Loch Linnhe Loop                  109\n8            Tortoise Street                  106\n9             Farm Pond Lane                  104\n"
     ]
    }
   ],
   "source": [
    "from modules.process_sql import sql_query\n",
    "\n",
    "db_file = 'sql\\\\Round_RockDb.db'\n",
    "\n",
    "sql_statement = r\"\"\"SELECT value, COUNT(wnUnion.value) as \"Unique Street Count\" \n",
    "                    FROM (SELECT key, value, type FROM ways_tags \n",
    "                          UNION ALL \n",
    "                          SELECT key, value, type FROM nodes_tags) as wnUnion\n",
    "                    WHERE key = 'street' AND type = 'addr'\n",
    "                    GROUP BY value\n",
    "                    ORDER BY \"Unique Street Count\" DESC\n",
    "                    Limit 10;\"\"\"\n",
    "\n",
    "sql_query(db_file, sql_statement)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Top 10 Postcodes Represented in Data\n",
    "\n",
    "The following are the results of the top 10 postcodes by occurence. The bolded are either on the border or have a small area in the Round Rock city limits. The results show that a majority of the postcode data represents the Round Rock area. Further future study will be needed to find out if they are businesses or residential.\n",
    "\n",
    "**Postcode**|   |**Count**\n",
    ":-----:| --- |:-----:\n",
    "78660|  |  1839\n",
    "**78613**|  |  **1744**\n",
    "78717|  |  755\n",
    "78664|  |  558\n",
    "78681|  |  460\n",
    "78728|  |  453\n",
    "**78729**|  |  **367**\n",
    "78634|  |  293\n",
    "**78641**|  |  **221**\n",
    "78665|  |  201\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "```\n",
    "    SELECT value, COUNT(wnUnion.value) as \"Unique Postcode Count\" \n",
    "                        FROM (SELECT key, value, type FROM ways_tags \n",
    "                            UNION ALL \n",
    "                            SELECT key, value, type FROM nodes_tags) as wnUnion\n",
    "                        WHERE key = 'postcode' AND type = 'addr'\n",
    "                        GROUP BY value\n",
    "                        ORDER BY \"Unique Postcode Count\" DESC\n",
    "                        Limit 10;\n",
    "```\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   value  Unique Postcode Count\n0  78660                   1839\n1  78613                   1744\n2  78717                    755\n3  78664                    558\n4  78681                    460\n5  78728                    453\n6  78729                    367\n7  78634                    293\n8  78641                    221\n9  78665                    201\n"
     ]
    }
   ],
   "source": [
    "from modules.process_sql import sql_query\n",
    "\n",
    "db_file = 'sql\\\\Round_RockDb.db'\n",
    "\n",
    "sql_statement = r\"\"\"SELECT value, COUNT(wnUnion.value) as \"Unique Postcode Count\" \n",
    "                    FROM (SELECT key, value, type FROM ways_tags \n",
    "                          UNION ALL \n",
    "                          SELECT key, value, type FROM nodes_tags) as wnUnion\n",
    "                    WHERE key = 'postcode' AND type = 'addr'\n",
    "                    GROUP BY value\n",
    "                    ORDER BY \"Unique Postcode Count\" DESC\n",
    "                    Limit 10;\"\"\"\n",
    "\n",
    "sql_query(db_file, sql_statement)"
   ]
  },
  {
   "source": [
    "### Duplicate Phone Numbers\n",
    "There were only 4 duplicated phone numbers in the data. The results are below. One of the duplicated numbers is for a middle school, one for a business wholesaler and the remaining two are for two CVS Pharmacy stores and two for Walgreens Pharmacy stores. This further leads to the conclusion that businesses may not be well represented in the data. Next I will check how many addresses contain I-35, SH 45, Louis Henna Boulevard and 620, which are all streets with primarily businesses.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Phone Number** |     |**Count**\n",
    ":-----           | --- |:-----:\n",
    "+1-512-428-2500  |     |  2\n",
    "+1-512-336-1328  |     |  2\n",
    "+1-512-310-8791  |     |  2\n",
    "+1-512-238-0475  |     |  2\n",
    "+1-956-648-8580  |     |  1\n",
    "+1-866-874-2389  |     |  1\n",
    "+1-866-583-7952  |     |  1\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "``` sql\n",
    "    SELECT value, COUNT(wnUnion.value) as \"Unique Phone Count\" \n",
    "                        FROM (SELECT key, value, type FROM ways_tags \n",
    "                            UNION ALL \n",
    "                            SELECT key, value, type FROM nodes_tags) as wnUnion\n",
    "                        WHERE key = 'phone' AND type = 'regular'\n",
    "                        GROUP BY value\n",
    "                        ORDER BY \"Unique Phone Count\" DESC\n",
    "                        Limit 10;\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "             value  Unique Phone Count\n0  +1-512-428-2500                   2\n1  +1-512-336-1328                   2\n2  +1-512-310-8791                   2\n3  +1-512-238-0475                   2\n4  +1-956-648-8580                   1\n5  +1-866-874-2389                   1\n6  +1-866-583-7952                   1\n7  +1-833-757-0636                   1\n8  +1-800-786-1000                   1\n9  +1-737-757-3100                   1\n"
     ]
    }
   ],
   "source": [
    "from modules.process_sql import sql_query\n",
    "\n",
    "db_file = 'sql\\\\Round_RockDb.db'\n",
    "\n",
    "sql_statement = r\"\"\"SELECT value, COUNT(wnUnion.value) as \"Unique Phone Count\" \n",
    "                    FROM (SELECT key, value, type FROM ways_tags \n",
    "                          UNION ALL \n",
    "                          SELECT key, value, type FROM nodes_tags) as wnUnion\n",
    "                    WHERE key = 'phone' AND type = 'regular'\n",
    "                    GROUP BY value\n",
    "                    ORDER BY \"Unique Phone Count\" DESC\n",
    "                    Limit 10;\"\"\"\n",
    "\n",
    "sql_query(db_file, sql_statement)"
   ]
  },
  {
   "source": [
    "### Addresses Containing I-35, SH 45, Louis Henna or 620\n",
    "The occurrences of street names with mostly business on them only show 248 records. Even more noticable from the query results below are that SH 45 shows only two business. This road is lined with businesses and should be much higher. The totals for I-35 and Ranch Road/Farm to Market 620 look more promising. There are likely several hundreds of businesses on each of these two roads but the totals for these two roads are better than SH 45. Louis Henna Boulevard's businesses are under represented. Knowing this area, there are likely over 100 businesses on this road.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Street** |       |**Occurances**\n",
    " -----:     | :-----:| -----:\n",
    "Ranch Road 620|  |82\n",
    "North I-35 |  |62\n",
    "South I-35 |  |29\n",
    "I-35 |  |22\n",
    "Louis Henna Boulevard|   |21\n",
    "West Louis Henna Boulevard|  |17\n",
    "North Ranch Road 620|  |9\n",
    "North Farm to Market 620| |4\n",
    "SH 45| |2  \n",
    "**Total**| |**248**\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "``` sql\n",
    "    SELECT value, COUNT(wnUnion.value) as \"Occurance Count\"\n",
    "                        FROM (SELECT key, value, type FROM ways_tags \n",
    "                            UNION ALL \n",
    "                            SELECT key, value, type FROM nodes_tags) as wnUnion\n",
    "                        WHERE (key = 'street' AND type = 'addr') AND\n",
    "                            (value LIKE '%I-35%' OR value LIKE '%SH 45%' OR value LIKE '%620%' or value LIKE '%Louis Henna%')\n",
    "                        GROUP BY value\n",
    "                        ORDER BY \"Occurance Count\" DESC\n",
    "                        Limit 20;\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                        value  Occurance Count\n",
      "0              Ranch Road 620               82\n",
      "1                  North I-35               62\n",
      "2                  South I-35               29\n",
      "3                        I-35               22\n",
      "4       Louis Henna Boulevard               21\n",
      "5  West Louis Henna Boulevard               17\n",
      "6        North Ranch Road 620                9\n",
      "7    North Farm to Market 620                4\n",
      "8                       SH 45                2\n"
     ]
    }
   ],
   "source": [
    "from modules.process_sql import sql_query\n",
    "\n",
    "db_file = 'sql\\\\Round_RockDb.db'\n",
    "\n",
    "sql_statement = r\"\"\"SELECT value, COUNT(wnUnion.value) as \"Occurance Count\"\n",
    "                    FROM (SELECT key, value, type FROM ways_tags \n",
    "                          UNION ALL \n",
    "                          SELECT key, value, type FROM nodes_tags) as wnUnion\n",
    "                    WHERE (key = 'street' AND type = 'addr') AND\n",
    "                          (value LIKE '%I-35%' OR value LIKE '%SH 45%' OR value LIKE '%620%' or value LIKE '%Louis Henna%')\n",
    "                    GROUP BY value\n",
    "                    ORDER BY \"Occurance Count\" DESC\n",
    "                    Limit 20;\"\"\"\n",
    "\n",
    "sql_query(db_file, sql_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Occurance Count\n0              248\n"
     ]
    }
   ],
   "source": [
    "from modules.process_sql import sql_query\n",
    "\n",
    "db_file = 'sql\\\\Round_RockDb.db'\n",
    "\n",
    "sql_statement = r\"\"\"SELECT COUNT(wnUnion.value) as \"Occurance Count\"\n",
    "                    FROM (SELECT key, value, type FROM ways_tags \n",
    "                          UNION ALL \n",
    "                          SELECT key, value, type FROM nodes_tags) as wnUnion\n",
    "                    WHERE (key = 'street' AND type = 'addr') AND\n",
    "                          (value LIKE '%I-35%' OR value LIKE '%SH 45%' OR value LIKE '%620%' or value LIKE '%Louis Henna%');\"\"\"\n",
    "\n",
    "sql_query(db_file, sql_statement)"
   ]
  },
  {
   "source": [
    "## Conclusion\n",
    "The purpose of this project was to begin exploring, auditing and cleaning the OpenStreetMaps data for Round Rock, Texas. The node and way tag attributes for streets, postcodes and phone numbers were chosen to focus on how well businesses are represented in the data. It was found that the data set does contain valid data for many businesses but is not close to a complete representation of the total businesses in the city of Round Rock. The data was found to have a high level of consistency and uniformity with the postcodes, phone numbers and street names. Though, there were instances of inconsistency, with all three attribute values they did not represent a majority of the values. There were 28,533 street entries, 7,110 postcode entries and 308 phone entries. The streets had a little over 100 corrections, the postcodes had less than 10, and the phone number corrections were less than 30. This supports that most of the data is consistent and uniform. Most important to me is the data allowed me to deepen my knowledge and skills at data wrangling by working with real world data and solving real world problems. As always, data sets can always be improved. Some ideas of ways to improve the data are given in the next section.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "        key  Occurrence Count\n0    street             28533\n1  postcode              7110\n2     phone               308\n"
     ]
    }
   ],
   "source": [
    "from modules.process_sql import sql_query\n",
    "\n",
    "db_file = 'sql\\\\Round_RockDb.db'\n",
    "\n",
    "sql_statement = r\"\"\"SELECT key, COUNT(*) as \"Occurrence Count\"\n",
    "                    FROM (SELECT key, value, type FROM ways_tags \n",
    "                          UNION ALL \n",
    "                          SELECT key, value, type FROM nodes_tags) as wnUnion\n",
    "                    WHERE (type = 'addr' AND (key = 'street' OR key = 'postcode')) OR\n",
    "                          (type = 'regular' AND key = 'phone')\n",
    "                    GROUP BY key\n",
    "                    ORDER BY \"Occurrence Count\" DESC;\"\"\"\n",
    "\n",
    "sql_query(db_file, sql_statement)"
   ]
  },
  {
   "source": [
    "### Additional Ideas for Improving the Data Including Benefits and Anticipated Problems\n",
    "Though most of the data is consistent and uniform, data values that should follow a strict pattern are not universally uniform. As an example, in the data there were postcodes for the US that were 6 digits or had the zip+4 format. These could be easily checked using the regex pattern `^\\d{5}$` before allowing them to be placed in the database. A similar pattern such as, `^\\+1-[2-9]\\d{2}-\\d{3}-\\d{4}$`, could be used to ensure consistency with phone numbers. If a pattern doesn't match, a user could be notified immediately and given information on formatting. Some anticipated problems with this improvement would be each country would need to have their own format pattern to check for and some mechanism would need to be in place to deal with any outliers that occur. An additional problem would be that being more strict tends to make some contributors stop contributing. Hopefully, the benefit of more consistent data would cause more businesses and developers of applications to those businesses to use the dataset. Therefore, canceling out the effect of the lost contributors. \n",
    "\n",
    "An additional thought is to make detailed standards for the naming conventions of common data values which can be programmatically checked similar to the audits done in this project. Place the standards prominently in an easily found place like the front-page menu bar. As an example of the challenge of finding a standard, while conducting a search for naming conventions I came across some OpenStreetMaps [Editing Standards and Conventions](https://wiki.openstreetmap.org/wiki/Editing_Standards_and_Conventions) through an external site but was unable to find other information on conventions used for phone numbers. When I finally found the conventions used for phone numbers, I found **two different methods** recommended as a standard. It is better to be consistent and not offer multiple ways of representing the same data. Only offer multiple choices when the data values are distinctly different such as with cuisines. Even then a menu of choices is better for the consistency and uniformity of the data. The query below gave 53 distinct cuisines found in the Round Rock data set. Some of the different representations, from the query, for the same items are:\n",
    "\n",
    "* italian_pizza, pizza\n",
    "* steak, steak_house\n",
    "* wings, Wings\n",
    "* american, American, local, regional\n",
    "* ...\n",
    "\n",
    "By offering a menu of items or at least a standard for naming items, these variations could be reduced making the data of a higher quality.\n",
    "\n",
    "```sql\n",
    "SELECT DISTINCT value\n",
    "        FROM (SELECT key, value, type FROM ways_tags \n",
    "                UNION ALL \n",
    "                SELECT key, value, type FROM nodes_tags) as wnUnion\n",
    "        WHERE (key = 'cuisine')\n",
    "        ORDER BY value;\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### References\n",
    "https://mungingdata.com/sqlite/create-database-load-csv-python/\n",
    "\n",
    "https://www.sqlitetutorial.net/sqlite-python/creating-database/\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html\n",
    "\n",
    "https://stackoverflow.com/questions/51463449/replace-csv-header-without-deleting-the-other-rows/51463964\n",
    "\n",
    "https://stackoverflow.com/questions/35486721/how-to-prevent-use-of-the-first-row-pandas-dataframe-as-column-names-when-using\n",
    "\n",
    "https://stackoverflow.com/questions/32213066/sqlite3-you-must-not-use-8-bit-bytestrings-unless-you-use-a-text-factory\n",
    "\n",
    "https://stackoverflow.com/questions/12817151/how-to-get-column-names-with-query-data-in-sqlite3\n",
    "\n",
    "https://maps.roundrocktexas.gov/cityview/\n",
    "\n",
    "https://marketbusinessnews.com/financial-glossary/zip-code/\n",
    "\n",
    "https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "\n",
    "https://thispointer.com/python-get-file-size-in-kb-mb-or-gb-human-readable-format/\n",
    "\n",
    "https://wiki.openstreetmap.org/wiki/Austin,_TX/Buildings_Import/Software_Setup\n",
    "\n",
    "https://wiki.openstreetmap.org/wiki/Editing_Standards_and_Conventions\n",
    "\n",
    "https://wiki.openstreetmap.org/wiki/Key:phone\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
